{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b411835c-7ade-4d79-b347-ead262ddf4dd",
   "metadata": {},
   "source": [
    "# Sarah Gets a Diamond - Starter Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4223203-0c5c-4cc7-80bc-81ca2bb0f662",
   "metadata": {},
   "source": [
    "In this code, we will build a model to predict diamond prices based on the features of the diamond. We will then compete as a class as to who can build the most predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dcad29-c77b-469d-a25d-7f3bc4eab4e0",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4976b23-96ae-4aa3-b13e-a81b17ab13ca",
   "metadata": {},
   "source": [
    "Like all of our starter code, we will start by loading a set of \"modules\" that contain useful functionality for the assignment. We will use most of these modules in every set of starter code we have, but we will customize the set for some assignment specific functionality. The way this works is we specify the name of a module first, and then we can either import it with a shorthand phrase of import certain functions.\n",
    "\n",
    "For example, the code `import numpy as np` says to import the numpy module (a module for handling large datasets as arrays that is very common in data science) and call it `np`. Then if we ever want to use a function from that module, for example the `array` function, we would type `np.array(x)` (where `x` is the data we are giving to the function).\n",
    "\n",
    "Alternatively, we can import specific functions directly. Below we write `from scipy.optimize import minimize_scalar`, which says to take \"from\" the `scipy.optimize` module the function `minimize_scalar`. This allows us to use the function `minimize_scalar` just by writing `minimize_scalar`.\n",
    "\n",
    "We can also import all functions from a module at a time like we do with `from math import *`. This imports all of the functions from math, which includes things like `log()` to calculate the natural logarithm of a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "893706c2-7bae-4cd3-bd58-4e20264d6022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import *\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7015625-4671-476d-a300-83c7cda1b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a comment. Anything in a \"code cell\" that is preceeded by a \"#\" is a comment\n",
    "# and it will not be interpreted as code to be run when you run the cell.\n",
    "# This sets some nicer defaults for plotting.\n",
    "# This must be run in a separate cell from importing matplotlib due to a bug.\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (11.0, 11.0),\n",
    "          'axes.labelsize': 'x-large',\n",
    "          'axes.titlesize':'xx-large',\n",
    "          'xtick.labelsize':'large',\n",
    "          'ytick.labelsize':'large'}\n",
    "mpl.rcParams.update(params)\n",
    "\n",
    "# This makes it so that the pandas dataframes don't get truncated horizontally.\n",
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea402a8-17b0-4234-9c6d-8f9830122e04",
   "metadata": {},
   "source": [
    "If you are uncertain what a function does, you can look up help in the jupyter notebook by writing `?np.array` (replace `np.array` with the relevant function). The next cell gives us the documentation for the `pandas` function, `read_csv`. Since we imported the `pandas` library as `pd` above, we reference that function by `pd.read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bd7e0ea-2c2f-4ebf-bddf-9ed0d5c639fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msep\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None | lib.NoDefault'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdelimiter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None | lib.NoDefault'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mheader\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"int | Sequence[int] | None | Literal['infer']\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnames\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Sequence[Hashable] | None | lib.NoDefault'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mindex_col\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'IndexLabel | Literal[False] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'DtypeArg | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'CSVEngine | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconverters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrue_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfalse_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mskipinitialspace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mskipfooter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnrows\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkeep_default_na\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mna_filter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mskip_blank_lines\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mparse_dates\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | Sequence[Hashable] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minfer_datetime_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | lib.NoDefault'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkeep_date_col\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdate_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdate_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdayfirst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcache_dates\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchunksize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'CompressionOptions'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mthousands\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdecimal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlineterminator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquotechar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquoting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdoublequote\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mescapechar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcomment\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding_errors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'strict'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdialect\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | csv.Dialect | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mon_bad_lines\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdelim_whitespace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmemory_map\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfloat_precision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Literal['high', 'legacy'] | None\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'StorageOptions'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype_backend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'DtypeBackend | lib.NoDefault'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'DataFrame | TextFileReader'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Read a comma-separated values (csv) file into DataFrame.\n",
       "\n",
       "Also supports optionally iterating or breaking of the file\n",
       "into chunks.\n",
       "\n",
       "Additional help can be found in the online docs for\n",
       "`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "filepath_or_buffer : str, path object or file-like object\n",
       "    Any valid string path is acceptable. The string could be a URL. Valid\n",
       "    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
       "    expected. A local file could be: file://localhost/path/to/table.csv.\n",
       "\n",
       "    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
       "\n",
       "    By file-like object, we refer to objects with a ``read()`` method, such as\n",
       "    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
       "sep : str, default ','\n",
       "    Delimiter to use. If sep is None, the C engine cannot automatically detect\n",
       "    the separator, but the Python parsing engine can, meaning the latter will\n",
       "    be used and automatically detect the separator by Python's builtin sniffer\n",
       "    tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n",
       "    different from ``'\\s+'`` will be interpreted as regular expressions and\n",
       "    will also force the use of the Python parsing engine. Note that regex\n",
       "    delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
       "delimiter : str, default ``None``\n",
       "    Alias for sep.\n",
       "header : int, list of int, None, default 'infer'\n",
       "    Row number(s) to use as the column names, and the start of the\n",
       "    data.  Default behavior is to infer the column names: if no names\n",
       "    are passed the behavior is identical to ``header=0`` and column\n",
       "    names are inferred from the first line of the file, if column\n",
       "    names are passed explicitly then the behavior is identical to\n",
       "    ``header=None``. Explicitly pass ``header=0`` to be able to\n",
       "    replace existing names. The header can be a list of integers that\n",
       "    specify row locations for a multi-index on the columns\n",
       "    e.g. [0,1,3]. Intervening rows that are not specified will be\n",
       "    skipped (e.g. 2 in this example is skipped). Note that this\n",
       "    parameter ignores commented lines and empty lines if\n",
       "    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
       "    data rather than the first line of the file.\n",
       "names : array-like, optional\n",
       "    List of column names to use. If the file contains a header row,\n",
       "    then you should explicitly pass ``header=0`` to override the column names.\n",
       "    Duplicates in this list are not allowed.\n",
       "index_col : int, str, sequence of int / str, or False, optional, default ``None``\n",
       "  Column(s) to use as the row labels of the ``DataFrame``, either given as\n",
       "  string name or column index. If a sequence of int / str is given, a\n",
       "  MultiIndex is used.\n",
       "\n",
       "  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
       "  column as the index, e.g. when you have a malformed file with delimiters at\n",
       "  the end of each line.\n",
       "usecols : list-like or callable, optional\n",
       "    Return a subset of the columns. If list-like, all elements must either\n",
       "    be positional (i.e. integer indices into the document columns) or strings\n",
       "    that correspond to column names provided either by the user in `names` or\n",
       "    inferred from the document header row(s). If ``names`` are given, the document\n",
       "    header row(s) are not taken into account. For example, a valid list-like\n",
       "    `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
       "    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
       "    To instantiate a DataFrame from ``data`` with element order preserved use\n",
       "    ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n",
       "    in ``['foo', 'bar']`` order or\n",
       "    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
       "    for ``['bar', 'foo']`` order.\n",
       "\n",
       "    If callable, the callable function will be evaluated against the column\n",
       "    names, returning names where the callable function evaluates to True. An\n",
       "    example of a valid callable argument would be ``lambda x: x.upper() in\n",
       "    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
       "    parsing time and lower memory usage.\n",
       "dtype : Type name or dict of column -> type, optional\n",
       "    Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32,\n",
       "    'c': 'Int64'}\n",
       "    Use `str` or `object` together with suitable `na_values` settings\n",
       "    to preserve and not interpret dtype.\n",
       "    If converters are specified, they will be applied INSTEAD\n",
       "    of dtype conversion.\n",
       "\n",
       "    .. versionadded:: 1.5.0\n",
       "\n",
       "        Support for defaultdict was added. Specify a defaultdict as input where\n",
       "        the default determines the dtype of the columns which are not explicitly\n",
       "        listed.\n",
       "engine : {'c', 'python', 'pyarrow'}, optional\n",
       "    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n",
       "    is currently more feature-complete. Multithreading is currently only supported by\n",
       "    the pyarrow engine.\n",
       "\n",
       "    .. versionadded:: 1.4.0\n",
       "\n",
       "        The \"pyarrow\" engine was added as an *experimental* engine, and some features\n",
       "        are unsupported, or may not work correctly, with this engine.\n",
       "converters : dict, optional\n",
       "    Dict of functions for converting values in certain columns. Keys can either\n",
       "    be integers or column labels.\n",
       "true_values : list, optional\n",
       "    Values to consider as True in addition to case-insensitive variants of \"True\".\n",
       "false_values : list, optional\n",
       "    Values to consider as False in addition to case-insensitive variants of \"False\".\n",
       "skipinitialspace : bool, default False\n",
       "    Skip spaces after delimiter.\n",
       "skiprows : list-like, int or callable, optional\n",
       "    Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
       "    at the start of the file.\n",
       "\n",
       "    If callable, the callable function will be evaluated against the row\n",
       "    indices, returning True if the row should be skipped and False otherwise.\n",
       "    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
       "skipfooter : int, default 0\n",
       "    Number of lines at bottom of file to skip (Unsupported with engine='c').\n",
       "nrows : int, optional\n",
       "    Number of rows of file to read. Useful for reading pieces of large files.\n",
       "na_values : scalar, str, list-like, or dict, optional\n",
       "    Additional strings to recognize as NA/NaN. If dict passed, specific\n",
       "    per-column NA values.  By default the following values are interpreted as\n",
       "    NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
       "    '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'None',\n",
       "    'n/a', 'nan', 'null'.\n",
       "keep_default_na : bool, default True\n",
       "    Whether or not to include the default NaN values when parsing the data.\n",
       "    Depending on whether `na_values` is passed in, the behavior is as follows:\n",
       "\n",
       "    * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n",
       "      is appended to the default NaN values used for parsing.\n",
       "    * If `keep_default_na` is True, and `na_values` are not specified, only\n",
       "      the default NaN values are used for parsing.\n",
       "    * If `keep_default_na` is False, and `na_values` are specified, only\n",
       "      the NaN values specified `na_values` are used for parsing.\n",
       "    * If `keep_default_na` is False, and `na_values` are not specified, no\n",
       "      strings will be parsed as NaN.\n",
       "\n",
       "    Note that if `na_filter` is passed in as False, the `keep_default_na` and\n",
       "    `na_values` parameters will be ignored.\n",
       "na_filter : bool, default True\n",
       "    Detect missing value markers (empty strings and the value of na_values). In\n",
       "    data without any NAs, passing na_filter=False can improve the performance\n",
       "    of reading a large file.\n",
       "verbose : bool, default False\n",
       "    Indicate number of NA values placed in non-numeric columns.\n",
       "skip_blank_lines : bool, default True\n",
       "    If True, skip over blank lines rather than interpreting as NaN values.\n",
       "parse_dates : bool or list of int or names or list of lists or dict, default False\n",
       "    The behavior is as follows:\n",
       "\n",
       "    * boolean. If True -> try parsing the index.\n",
       "    * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n",
       "      each as a separate date column.\n",
       "    * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n",
       "      a single date column.\n",
       "    * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n",
       "      result 'foo'\n",
       "\n",
       "    If a column or index cannot be represented as an array of datetimes,\n",
       "    say because of an unparsable value or a mixture of timezones, the column\n",
       "    or index will be returned unaltered as an object data type. For\n",
       "    non-standard datetime parsing, use ``pd.to_datetime`` after\n",
       "    ``pd.read_csv``.\n",
       "\n",
       "    Note: A fast-path exists for iso8601-formatted dates.\n",
       "infer_datetime_format : bool, default False\n",
       "    If True and `parse_dates` is enabled, pandas will attempt to infer the\n",
       "    format of the datetime strings in the columns, and if it can be inferred,\n",
       "    switch to a faster method of parsing them. In some cases this can increase\n",
       "    the parsing speed by 5-10x.\n",
       "\n",
       "    .. deprecated:: 2.0.0\n",
       "        A strict version of this argument is now the default, passing it has no effect.\n",
       "\n",
       "keep_date_col : bool, default False\n",
       "    If True and `parse_dates` specifies combining multiple columns then\n",
       "    keep the original columns.\n",
       "date_parser : function, optional\n",
       "    Function to use for converting a sequence of string columns to an array of\n",
       "    datetime instances. The default uses ``dateutil.parser.parser`` to do the\n",
       "    conversion. Pandas will try to call `date_parser` in three different ways,\n",
       "    advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
       "    (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n",
       "    string values from the columns defined by `parse_dates` into a single array\n",
       "    and pass that; and 3) call `date_parser` once for each row using one or\n",
       "    more strings (corresponding to the columns defined by `parse_dates`) as\n",
       "    arguments.\n",
       "\n",
       "    .. deprecated:: 2.0.0\n",
       "       Use ``date_format`` instead, or read in as ``object`` and then apply\n",
       "       :func:`to_datetime` as-needed.\n",
       "date_format : str or dict of column -> format, default ``None``\n",
       "   If used in conjunction with ``parse_dates``, will parse dates according to this\n",
       "   format. For anything more complex,\n",
       "   please read in as ``object`` and then apply :func:`to_datetime` as-needed.\n",
       "\n",
       "   .. versionadded:: 2.0.0\n",
       "dayfirst : bool, default False\n",
       "    DD/MM format dates, international and European format.\n",
       "cache_dates : bool, default True\n",
       "    If True, use a cache of unique, converted dates to apply the datetime\n",
       "    conversion. May produce significant speed-up when parsing duplicate\n",
       "    date strings, especially ones with timezone offsets.\n",
       "\n",
       "iterator : bool, default False\n",
       "    Return TextFileReader object for iteration or getting chunks with\n",
       "    ``get_chunk()``.\n",
       "\n",
       "    .. versionchanged:: 1.2\n",
       "\n",
       "       ``TextFileReader`` is a context manager.\n",
       "chunksize : int, optional\n",
       "    Return TextFileReader object for iteration.\n",
       "    See the `IO Tools docs\n",
       "    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
       "    for more information on ``iterator`` and ``chunksize``.\n",
       "\n",
       "    .. versionchanged:: 1.2\n",
       "\n",
       "       ``TextFileReader`` is a context manager.\n",
       "compression : str or dict, default 'infer'\n",
       "    For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n",
       "    path-like, then detect compression from the following extensions: '.gz',\n",
       "    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
       "    (otherwise no compression).\n",
       "    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n",
       "    Set to ``None`` for no decompression.\n",
       "    Can also be a dict with key ``'method'`` set\n",
       "    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'tar'``} and other\n",
       "    key-value pairs are forwarded to\n",
       "    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
       "    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor`` or\n",
       "    ``tarfile.TarFile``, respectively.\n",
       "    As an example, the following could be passed for Zstandard decompression using a\n",
       "    custom compression dictionary:\n",
       "    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n",
       "\n",
       "    .. versionadded:: 1.5.0\n",
       "        Added support for `.tar` files.\n",
       "\n",
       "    .. versionchanged:: 1.4.0 Zstandard support.\n",
       "\n",
       "thousands : str, optional\n",
       "    Thousands separator.\n",
       "decimal : str, default '.'\n",
       "    Character to recognize as decimal point (e.g. use ',' for European data).\n",
       "lineterminator : str (length 1), optional\n",
       "    Character to break file into lines. Only valid with C parser.\n",
       "quotechar : str (length 1), optional\n",
       "    The character used to denote the start and end of a quoted item. Quoted\n",
       "    items can include the delimiter and it will be ignored.\n",
       "quoting : int or csv.QUOTE_* instance, default 0\n",
       "    Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
       "    QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
       "doublequote : bool, default ``True``\n",
       "   When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n",
       "   whether or not to interpret two consecutive quotechar elements INSIDE a\n",
       "   field as a single ``quotechar`` element.\n",
       "escapechar : str (length 1), optional\n",
       "    One-character string used to escape other characters.\n",
       "comment : str, optional\n",
       "    Indicates remainder of line should not be parsed. If found at the beginning\n",
       "    of a line, the line will be ignored altogether. This parameter must be a\n",
       "    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
       "    fully commented lines are ignored by the parameter `header` but not by\n",
       "    `skiprows`. For example, if ``comment='#'``, parsing\n",
       "    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n",
       "    treated as the header.\n",
       "encoding : str, optional, default \"utf-8\"\n",
       "    Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n",
       "    standard encodings\n",
       "    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
       "\n",
       "    .. versionchanged:: 1.2\n",
       "\n",
       "       When ``encoding`` is ``None``, ``errors=\"replace\"`` is passed to\n",
       "       ``open()``. Otherwise, ``errors=\"strict\"`` is passed to ``open()``.\n",
       "       This behavior was previously only the case for ``engine=\"python\"``.\n",
       "\n",
       "    .. versionchanged:: 1.3.0\n",
       "\n",
       "       ``encoding_errors`` is a new argument. ``encoding`` has no longer an\n",
       "       influence on how encoding errors are handled.\n",
       "\n",
       "encoding_errors : str, optional, default \"strict\"\n",
       "    How encoding errors are treated. `List of possible values\n",
       "    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
       "\n",
       "    .. versionadded:: 1.3.0\n",
       "\n",
       "dialect : str or csv.Dialect, optional\n",
       "    If provided, this parameter will override values (default or not) for the\n",
       "    following parameters: `delimiter`, `doublequote`, `escapechar`,\n",
       "    `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n",
       "    override values, a ParserWarning will be issued. See csv.Dialect\n",
       "    documentation for more details.\n",
       "on_bad_lines : {'error', 'warn', 'skip'} or callable, default 'error'\n",
       "    Specifies what to do upon encountering a bad line (a line with too many fields).\n",
       "    Allowed values are :\n",
       "\n",
       "        - 'error', raise an Exception when a bad line is encountered.\n",
       "        - 'warn', raise a warning when a bad line is encountered and skip that line.\n",
       "        - 'skip', skip bad lines without raising or warning when they are encountered.\n",
       "\n",
       "    .. versionadded:: 1.3.0\n",
       "\n",
       "    .. versionadded:: 1.4.0\n",
       "\n",
       "        - callable, function with signature\n",
       "          ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n",
       "          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n",
       "          If the function returns ``None``, the bad line will be ignored.\n",
       "          If the function returns a new list of strings with more elements than\n",
       "          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n",
       "          Only supported when ``engine=\"python\"``\n",
       "\n",
       "delim_whitespace : bool, default False\n",
       "    Specifies whether or not whitespace (e.g. ``' '`` or ``'    '``) will be\n",
       "    used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n",
       "    is set to True, nothing should be passed in for the ``delimiter``\n",
       "    parameter.\n",
       "low_memory : bool, default True\n",
       "    Internally process the file in chunks, resulting in lower memory use\n",
       "    while parsing, but possibly mixed type inference.  To ensure no mixed\n",
       "    types either set False, or specify the type with the `dtype` parameter.\n",
       "    Note that the entire file is read into a single DataFrame regardless,\n",
       "    use the `chunksize` or `iterator` parameter to return the data in chunks.\n",
       "    (Only valid with C parser).\n",
       "memory_map : bool, default False\n",
       "    If a filepath is provided for `filepath_or_buffer`, map the file object\n",
       "    directly onto memory and access the data directly from there. Using this\n",
       "    option can improve performance because there is no longer any I/O overhead.\n",
       "float_precision : str, optional\n",
       "    Specifies which converter the C engine should use for floating-point\n",
       "    values. The options are ``None`` or 'high' for the ordinary converter,\n",
       "    'legacy' for the original lower precision pandas converter, and\n",
       "    'round_trip' for the round-trip converter.\n",
       "\n",
       "    .. versionchanged:: 1.2\n",
       "\n",
       "storage_options : dict, optional\n",
       "    Extra options that make sense for a particular storage connection, e.g.\n",
       "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
       "    are forwarded to ``urllib.request.Request`` as header options. For other\n",
       "    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
       "    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
       "    details, and for more examples on storage options refer `here\n",
       "    <https://pandas.pydata.org/docs/user_guide/io.html?\n",
       "    highlight=storage_options#reading-writing-remote-files>`_.\n",
       "\n",
       "    .. versionadded:: 1.2\n",
       "\n",
       "dtype_backend : {\"numpy_nullable\", \"pyarrow\"}, defaults to NumPy backed DataFrames\n",
       "    Which dtype_backend to use, e.g. whether a DataFrame should have NumPy\n",
       "    arrays, nullable dtypes are used for all dtypes that have a nullable\n",
       "    implementation when \"numpy_nullable\" is set, pyarrow is used for all\n",
       "    dtypes if \"pyarrow\" is set.\n",
       "\n",
       "    The dtype_backends are still experimential.\n",
       "\n",
       "    .. versionadded:: 2.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       "DataFrame or TextFileReader\n",
       "    A comma-separated values (csv) file is returned as two-dimensional\n",
       "    data structure with labeled axes.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
       "read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
       "read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?pd.read_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29717fcf-6519-46bd-b23d-b0dfb784dc29",
   "metadata": {},
   "source": [
    "Sometimes the documentation that it produces helps you to figure out what you need to put in the function (you can see it gives you some examples), but sometimes, it's still quite hard to parse. In these cases, I recommend you google the function with a specific question (or an error message if you are getting an error message), and you will likely find an explanation. The website [stackoverflow.com](http://www.stackoverflow.com) is a website that you will likely become very familiar with. It is a community driven question and answering platform for coding related questions. It is also often the most useful (and top) search result that comes from googling something."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476a41c-3ae6-4f84-bbac-0010fd8c70b4",
   "metadata": {},
   "source": [
    "## Load and clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec3ae32-9b78-4205-954c-0ea17017bceb",
   "metadata": {},
   "source": [
    "We first have to load the data. We will read it in as a Pandas \"dataframe\". Dataframes are one of the most important tools for a data scientist. They store data in a structured format, kind of like an excel spreadsheet.\n",
    "\n",
    "We can reference the data set with \"train.csv\" and the test set with \"test.csv\" because our Juptyer Notebook is in the same folder as the data set. If the notebook was in a different folder, we would have to provide a longer path to the file. If you ever create your own notebook and try to read in a file (or you upload new data to use in a Jupyter Notebook), you will either have to put the notebook and the data set in the same folder, or you will have to give it a more complete path to the data set. We will see examples of this later in the course.\n",
    "\n",
    "We store the training data in a \"variable\" called `df_train` and the testing data in a variable called `df_test`, and anytime we want to reference the data, we will use that variable. This convention will be fairly common for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d82e3d9-4758-4197-b2a8-383e3def40b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\", index_col=\"ID\")\n",
    "df_test = pd.read_csv(\"test.csv\", index_col=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b6d69-2b93-4048-baaf-2e69f19a408c",
   "metadata": {},
   "source": [
    "Notice that when we called the function `pd.read_csv`, we gave it (or _called_ it with) two _arguments_. The first argument is called a positional argument (i.e., the function knows what it is based on the position, namely that it is first, in the function call). This argument is the name of the dataset to read in.\n",
    "\n",
    "The second argument is called a _keyword_ argument because we called the function with a keyword set equal to something. In this case, we used the keyword `index_col` and we set it equal to the value `\"ID\"`. You can look back up at the function documentation from above (the `?pd.read_csv` cell) to see what the argument does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc9982e-6155-47a2-ab74-8ba5143db748",
   "metadata": {},
   "source": [
    "We can find the part of the documentation and read what that argument does, but I've included it below for easier reading (but it is just copy and pasted from above).\n",
    "\n",
    "```\n",
    "index_col : int, str, sequence of int / str, or False, default ``None``\n",
    "  Column(s) to use as the row labels of the ``DataFrame``, either given as\n",
    "  string name or column index. If a sequence of int / str is given, a\n",
    "  MultiIndex is used.\n",
    "\n",
    "  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
    "  column as the index, e.g. when you have a malformed file with delimiters at\n",
    "  the end of each line.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988223e4-580e-49c5-8ec7-cb16005512e6",
   "metadata": {},
   "source": [
    "Looking at the documentation, we can see that `index_col` gives each row of our data a name, and specifically the name it uses comes from the column named \"ID\" in the original csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa74f1-65f1-4cc8-a3fa-6a8305a3319e",
   "metadata": {},
   "source": [
    "Let's look at the first few rows. We use a built in `method` (a function that is specific to a certain kind of variable, in this case a pandas dataframe) called `head`. This will give a view of the dataframe that should look very familiar to any excel jockey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08b5b4b0-1863-4759-8f20-960d67c069c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Carat Weight</th>\n",
       "      <th>Cut</th>\n",
       "      <th>Color</th>\n",
       "      <th>Clarity</th>\n",
       "      <th>Polish</th>\n",
       "      <th>Symmetry</th>\n",
       "      <th>Report</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.10</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>VG</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "      <td>$5,169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.83</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>H</td>\n",
       "      <td>VS1</td>\n",
       "      <td>ID</td>\n",
       "      <td>ID</td>\n",
       "      <td>AGSL</td>\n",
       "      <td>$3,470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.85</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>EX</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "      <td>$3,183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.91</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>VG</td>\n",
       "      <td>VG</td>\n",
       "      <td>GIA</td>\n",
       "      <td>$4,370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.83</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>G</td>\n",
       "      <td>SI1</td>\n",
       "      <td>EX</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "      <td>$3,171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Carat Weight    Cut Color Clarity Polish Symmetry Report     Price\n",
       "ID                                                                    \n",
       "1           1.10  Ideal     H     SI1     VG       EX    GIA   $5,169 \n",
       "2           0.83  Ideal     H     VS1     ID       ID   AGSL   $3,470 \n",
       "3           0.85  Ideal     H     SI1     EX       EX    GIA   $3,183 \n",
       "4           0.91  Ideal     E     SI1     VG       VG    GIA   $4,370 \n",
       "5           0.83  Ideal     G     SI1     EX       EX    GIA   $3,171 "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6da730-13da-46e4-b60b-8ab198573130",
   "metadata": {},
   "source": [
    "What does `head` do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7552218d-5f32-4e12-83fd-ffaea5e82ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'NDFrameT'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return the first `n` rows.\n",
       "\n",
       "This function returns the first `n` rows for the object based\n",
       "on position. It is useful for quickly testing if your object\n",
       "has the right type of data in it.\n",
       "\n",
       "For negative values of `n`, this function returns all rows except\n",
       "the last `|n|` rows, equivalent to ``df[:n]``.\n",
       "\n",
       "If n is larger than the number of rows, this function returns all rows.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n : int, default 5\n",
       "    Number of rows to select.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "same type as caller\n",
       "    The first `n` rows of the caller object.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "DataFrame.tail: Returns the last `n` rows.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion',\n",
       "...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n",
       ">>> df\n",
       "      animal\n",
       "0  alligator\n",
       "1        bee\n",
       "2     falcon\n",
       "3       lion\n",
       "4     monkey\n",
       "5     parrot\n",
       "6      shark\n",
       "7      whale\n",
       "8      zebra\n",
       "\n",
       "Viewing the first 5 lines\n",
       "\n",
       ">>> df.head()\n",
       "      animal\n",
       "0  alligator\n",
       "1        bee\n",
       "2     falcon\n",
       "3       lion\n",
       "4     monkey\n",
       "\n",
       "Viewing the first `n` lines (three in this case)\n",
       "\n",
       ">>> df.head(3)\n",
       "      animal\n",
       "0  alligator\n",
       "1        bee\n",
       "2     falcon\n",
       "\n",
       "For negative values of `n`\n",
       "\n",
       ">>> df.head(-3)\n",
       "      animal\n",
       "0  alligator\n",
       "1        bee\n",
       "2     falcon\n",
       "3       lion\n",
       "4     monkey\n",
       "5     parrot\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?df_train.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01684bc-0bd4-4d6d-bacf-d96327378a2d",
   "metadata": {},
   "source": [
    "Note that we didn't give the function `df_train.head()` any arguments, but we could have. It takes up to one argument called `n`, so we could have also called the function like as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91c9bf46-a818-4726-b9ec-d50afb8bc6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Carat Weight</th>\n",
       "      <th>Cut</th>\n",
       "      <th>Color</th>\n",
       "      <th>Clarity</th>\n",
       "      <th>Polish</th>\n",
       "      <th>Symmetry</th>\n",
       "      <th>Report</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6001</th>\n",
       "      <td>2.18</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>I</td>\n",
       "      <td>SI1</td>\n",
       "      <td>EX</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6002</th>\n",
       "      <td>2.32</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>VVS2</td>\n",
       "      <td>VG</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6003</th>\n",
       "      <td>0.77</td>\n",
       "      <td>Good</td>\n",
       "      <td>F</td>\n",
       "      <td>VS1</td>\n",
       "      <td>VG</td>\n",
       "      <td>G</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6004</th>\n",
       "      <td>2.01</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>G</td>\n",
       "      <td>VS2</td>\n",
       "      <td>EX</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6005</th>\n",
       "      <td>1.39</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VVS2</td>\n",
       "      <td>VG</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6006</th>\n",
       "      <td>1.22</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>H</td>\n",
       "      <td>VS1</td>\n",
       "      <td>ID</td>\n",
       "      <td>ID</td>\n",
       "      <td>AGSL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6007</th>\n",
       "      <td>1.01</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>G</td>\n",
       "      <td>VG</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6008</th>\n",
       "      <td>1.50</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>F</td>\n",
       "      <td>SI1</td>\n",
       "      <td>VG</td>\n",
       "      <td>VG</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6009</th>\n",
       "      <td>0.90</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>SI1</td>\n",
       "      <td>EX</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6010</th>\n",
       "      <td>2.11</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VVS2</td>\n",
       "      <td>EX</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Carat Weight        Cut Color Clarity Polish Symmetry Report\n",
       "ID                                                                \n",
       "6001          2.18  Very Good     I     SI1     EX       EX    GIA\n",
       "6002          2.32      Ideal     E    VVS2     VG       EX    GIA\n",
       "6003          0.77       Good     F     VS1     VG        G    GIA\n",
       "6004          2.01  Very Good     G     VS2     EX       EX    GIA\n",
       "6005          1.39  Very Good     E    VVS2     VG       EX    GIA\n",
       "6006          1.22      Ideal     H     VS1     ID       ID   AGSL\n",
       "6007          1.01  Very Good     E     VS1      G       VG    GIA\n",
       "6008          1.50      Ideal     F     SI1     VG       VG    GIA\n",
       "6009          0.90  Very Good     F     SI1     EX       EX    GIA\n",
       "6010          2.11  Very Good     E    VVS2     EX       EX    GIA"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8771960-6bbb-4dfa-9ad4-c6a9c6d2384c",
   "metadata": {},
   "source": [
    "The function is only given one argument, and since there is only one position, it knows the argument is the `n` argument. We could also give it the name (or keyword) of the argument as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe938c85-0367-4edf-8134-8163cde48c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Carat Weight</th>\n",
       "      <th>Cut</th>\n",
       "      <th>Color</th>\n",
       "      <th>Clarity</th>\n",
       "      <th>Polish</th>\n",
       "      <th>Symmetry</th>\n",
       "      <th>Report</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6001</th>\n",
       "      <td>2.18</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>I</td>\n",
       "      <td>SI1</td>\n",
       "      <td>EX</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6002</th>\n",
       "      <td>2.32</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>VVS2</td>\n",
       "      <td>VG</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6003</th>\n",
       "      <td>0.77</td>\n",
       "      <td>Good</td>\n",
       "      <td>F</td>\n",
       "      <td>VS1</td>\n",
       "      <td>VG</td>\n",
       "      <td>G</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6004</th>\n",
       "      <td>2.01</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>G</td>\n",
       "      <td>VS2</td>\n",
       "      <td>EX</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6005</th>\n",
       "      <td>1.39</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VVS2</td>\n",
       "      <td>VG</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6006</th>\n",
       "      <td>1.22</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>H</td>\n",
       "      <td>VS1</td>\n",
       "      <td>ID</td>\n",
       "      <td>ID</td>\n",
       "      <td>AGSL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6007</th>\n",
       "      <td>1.01</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>G</td>\n",
       "      <td>VG</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6008</th>\n",
       "      <td>1.50</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>F</td>\n",
       "      <td>SI1</td>\n",
       "      <td>VG</td>\n",
       "      <td>VG</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6009</th>\n",
       "      <td>0.90</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>SI1</td>\n",
       "      <td>EX</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6010</th>\n",
       "      <td>2.11</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VVS2</td>\n",
       "      <td>EX</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6011</th>\n",
       "      <td>2.12</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>VG</td>\n",
       "      <td>VG</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6012</th>\n",
       "      <td>1.05</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>G</td>\n",
       "      <td>VS1</td>\n",
       "      <td>EX</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Carat Weight        Cut Color Clarity Polish Symmetry Report\n",
       "ID                                                                \n",
       "6001          2.18  Very Good     I     SI1     EX       EX    GIA\n",
       "6002          2.32      Ideal     E    VVS2     VG       EX    GIA\n",
       "6003          0.77       Good     F     VS1     VG        G    GIA\n",
       "6004          2.01  Very Good     G     VS2     EX       EX    GIA\n",
       "6005          1.39  Very Good     E    VVS2     VG       EX    GIA\n",
       "6006          1.22      Ideal     H     VS1     ID       ID   AGSL\n",
       "6007          1.01  Very Good     E     VS1      G       VG    GIA\n",
       "6008          1.50      Ideal     F     SI1     VG       VG    GIA\n",
       "6009          0.90  Very Good     F     SI1     EX       EX    GIA\n",
       "6010          2.11  Very Good     E    VVS2     EX       EX    GIA\n",
       "6011          2.12  Very Good     H     SI1     VG       VG    GIA\n",
       "6012          1.05  Very Good     G     VS1     EX       EX    GIA"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(n=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f699a7b-8daa-497d-9f07-56ba2379b79f",
   "metadata": {},
   "source": [
    "We can see that `n` controls how many rows we get back. We can also see that the index (`ID`) of the `df_test` starts at `6001` instead of `1` because the ID of the rows in our test set start at `6001`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc328ec-f41f-4ad5-b1f0-bee7a530be15",
   "metadata": {},
   "source": [
    "How big are our dataset? We can use another variable specific value called `shape`. Now this is a property (something that just is for the dataframe), not a function. So, we don't put `()` after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f61b4d8-dd2c-47d4-b2ef-46d429b88549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1cc3763-5d00-4c92-aec4-ef755ce39dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3142, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52b00f-4b29-42d9-9925-4760ea2cdf9c",
   "metadata": {},
   "source": [
    "What does it actually tell us? Well, the first number is the number of rows, and the second is the number of columns. So, in our training set we have 6000 diamonds in our data set, and 8 pieces of data about each diamond. Similarly, for the test data set, we have 3142 diamonds and seven pieces of data. Note that we have one fewer pieces of data in our testing set. Think about why for a moment... It's because we don't have prices in the testing set, we will be predicting on the testing set and submitting our predictions for the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a695e-ee8d-4324-a79c-cc312d6b5c5c",
   "metadata": {},
   "source": [
    "Lett's take a closer look at our dataframe to see what is actually in our data. Below, I define a \"function\" that takes in a dataframe and spits out nice summary of a bunch of different factors for each column in the dataframe. We will use this function in nearly every assignment we have, but we don't have to rewrite it. We can just copy and paste it to our new code. Also, we only have to define it once. Once it has been defined, we can use it in the rest of our code. Let's use it to get a better understanding of this particular dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcb26e26-156b-458b-8fad-b958905fe9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dataframe(df):\n",
    "    \"\"\"Summarize a dataframe, and report missing values.\"\"\"\n",
    "    missing_values = pd.concat([pd.DataFrame(df.columns, columns=['Variable Name']), \n",
    "                      pd.DataFrame(df.dtypes.values.reshape([-1,1]), columns=['Data Type']),\n",
    "                      pd.DataFrame(df.isnull().sum().values, columns=['Missing Values']), \n",
    "                      pd.DataFrame([df[name].nunique() for name in df.columns], columns=['Unique Values'])], \n",
    "                     axis=1).set_index('Variable Name')\n",
    "    with pd.option_context(\"display.max_rows\", 1000):\n",
    "        display(pd.concat([missing_values, df.describe(include='all').transpose()], axis=1).fillna(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "618c306b-ecc1-4202-825f-9a3d909bd4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>Unique Values</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Carat Weight</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>6000.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.33452</td>\n",
       "      <td>0.475696</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.59</td>\n",
       "      <td>2.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cut</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>2482</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6</td>\n",
       "      <td>G</td>\n",
       "      <td>1501</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clarity</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>7</td>\n",
       "      <td>SI1</td>\n",
       "      <td>2059</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polish</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>EX</td>\n",
       "      <td>2425</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Symmetry</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>VG</td>\n",
       "      <td>2417</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Report</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>GIA</td>\n",
       "      <td>5266</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>4821</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>4821</td>\n",
       "      <td>$4,466</td>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Data Type  Missing Values  Unique Values   count unique  \\\n",
       "Carat Weight   float64               0            196  6000.0          \n",
       "Cut             object               0              5  6000.0      5   \n",
       "Color           object               0              6  6000.0      6   \n",
       "Clarity         object               0              7  6000.0      7   \n",
       "Polish          object               0              4  6000.0      4   \n",
       "Symmetry        object               0              4  6000.0      4   \n",
       "Report          object               0              2  6000.0      2   \n",
       "Price           object               0           4821  6000.0   4821   \n",
       "\n",
       "                   top  freq     mean       std   min  25%   50%   75%   max  \n",
       "Carat Weight                  1.33452  0.475696  0.75  1.0  1.13  1.59  2.91  \n",
       "Cut              Ideal  2482                                                  \n",
       "Color                G  1501                                                  \n",
       "Clarity            SI1  2059                                                  \n",
       "Polish              EX  2425                                                  \n",
       "Symmetry            VG  2417                                                  \n",
       "Report             GIA  5266                                                  \n",
       "Price          $4,466      8                                                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarize_dataframe(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "481edf32-e5ed-481e-b691-9272a555de14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>Unique Values</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Carat Weight</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>3142.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.33923</td>\n",
       "      <td>0.47834</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.61</td>\n",
       "      <td>2.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cut</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3142.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>1301</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3142.0</td>\n",
       "      <td>6</td>\n",
       "      <td>G</td>\n",
       "      <td>752</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clarity</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3142.0</td>\n",
       "      <td>6</td>\n",
       "      <td>SI1</td>\n",
       "      <td>1051</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polish</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3142.0</td>\n",
       "      <td>4</td>\n",
       "      <td>EX</td>\n",
       "      <td>1279</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Symmetry</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3142.0</td>\n",
       "      <td>4</td>\n",
       "      <td>VG</td>\n",
       "      <td>1250</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Report</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3142.0</td>\n",
       "      <td>2</td>\n",
       "      <td>GIA</td>\n",
       "      <td>2757</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Data Type  Missing Values  Unique Values   count unique    top  \\\n",
       "Carat Weight   float64               0            172  3142.0                 \n",
       "Cut             object               0              5  3142.0      5  Ideal   \n",
       "Color           object               0              6  3142.0      6      G   \n",
       "Clarity         object               0              6  3142.0      6    SI1   \n",
       "Polish          object               0              4  3142.0      4     EX   \n",
       "Symmetry        object               0              4  3142.0      4     VG   \n",
       "Report          object               0              2  3142.0      2    GIA   \n",
       "\n",
       "              freq     mean      std   min   25%   50%   75%   max  \n",
       "Carat Weight        1.33923  0.47834  0.75  1.01  1.13  1.61  2.79  \n",
       "Cut           1301                                                  \n",
       "Color          752                                                  \n",
       "Clarity       1051                                                  \n",
       "Polish        1279                                                  \n",
       "Symmetry      1250                                                  \n",
       "Report        2757                                                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarize_dataframe(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac8b67-58f8-4519-bef3-76965784d7e5",
   "metadata": {},
   "source": [
    "We can see that in both our training and testing data, we don't have any missing data. This will greatly simplify our data preparation. We can also see the type of each data. Generally, the most common data types we'll see are `float64`, which is just a number, and `object`, which is basically anything that is not a number. The most common example of an `object` is a piece of data that is text based, and it generally means that the data is _categorical_. In this case we can see how many categories each data type has by lookin at the column \"Unique Values\". For example, the are five unique values for \"Cut\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44897f34-0f59-492e-b8e6-9306a817881d",
   "metadata": {},
   "source": [
    "We can see that \"Price\" is also an `object` type for our training data set. This is a little unexpected because \"Price\" should just be a number. Let's take a look at what is in that column. We can do that by _indexing_ into our dataframe to get back just the \"Price\" column like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "387480c2-57ef-4298-93c4-63de158870ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1         $5,169 \n",
       "2         $3,470 \n",
       "3         $3,183 \n",
       "4         $4,370 \n",
       "5         $3,171 \n",
       "          ...    \n",
       "5996      $6,250 \n",
       "5997      $5,328 \n",
       "5998      $6,157 \n",
       "5999     $11,206 \n",
       "6000     $30,507 \n",
       "Name: Price, Length: 6000, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf33fd6-2901-4be9-8de7-1be913ff356c",
   "metadata": {},
   "source": [
    "We can then immediately see what the problem is. The \"Price\" column includes a dollar sign \"$\" and a comma. That made the computer guess that the data in the column is an object. The computer doesn't know what type these pieces of data are supposed to be, so it has make it's best guess when it reads it in. It has gotten it wrong here, and we need to fix it in order to successfully build our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eae163-de5a-42c5-a7c7-54624d4ac9df",
   "metadata": {},
   "source": [
    "Well, the code to do that is obviously the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af44c433-27ec-477e-9d12-a79241953284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1        5169.0\n",
       "2        3470.0\n",
       "3        3183.0\n",
       "4        4370.0\n",
       "5        3171.0\n",
       "         ...   \n",
       "5996     6250.0\n",
       "5997     5328.0\n",
       "5998     6157.0\n",
       "5999    11206.0\n",
       "6000    30507.0\n",
       "Name: Price, Length: 6000, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Price'].replace(to_replace='[\\$,]', value='', regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a302612c-c988-4406-9557-56cc0b18c771",
   "metadata": {},
   "source": [
    "We can see at the bottom that the data type (or `dtype`) is now `float64`, the number we were looking for.\n",
    "\n",
    "However, the code above was in no way obvious, particularly when you are new to python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48792643-39d5-4a46-9f92-86bda1a00e3f",
   "metadata": {},
   "source": [
    "### Aside: How would I figure that piece of code out?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d576f9-5deb-44c1-9945-02116689480b",
   "metadata": {},
   "source": [
    "As an exercise, let's walk through how one would go about figuring out both how to convert a column of data that includes dollar signs and commas into a numerical value. This is not something that you should expect to know off the top of your head or memorize. You should instead learn how to re-discover this kind of step for yourself when necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d9c068-4dd9-4aae-8845-a43815be3718",
   "metadata": {},
   "source": [
    "The first thing I did was google \"convert dollar value to numeric pandas.\" The first result was the following [Stack overflow article](https://stackoverflow.com/questions/32464280/converting-currency-with-to-numbers-in-python-pandas). Take a moment and read the question and the first answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2325889c-fa65-4682-b2f8-f4c8578b2613",
   "metadata": {},
   "source": [
    "The question is asking almost for what we are trying to do, but in the question there are multiple columns that they need to convert (all except for the first column). So, we need to modify their answer a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea002fa-c2f4-4e8b-87c3-4302414f52c0",
   "metadata": {},
   "source": [
    "Since they are trying to convert multiple columns, they use the piece of code `df.columns[1:]` instead of the `'Price'` that was in our piece of code. Let's see what that does. Note that they're dataframe is called `df` while ours is called `df_train`, so we'll have to use `df_train.columns[1:]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b4ff6e6-c790-4393-a6a9-03f1ec49eee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Cut', 'Color', 'Clarity', 'Polish', 'Symmetry', 'Report', 'Price'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd822b18-166f-4479-a43c-7bc90817927e",
   "metadata": {},
   "source": [
    "Ah, it gives us a list of the column names, except for the first column (which is \"Carat Weight\" in our data set). So, since we only want to convert a single column, then we can just type in a single column. That's why we use `df_train['Price']` instead of `df_train[df_train.columns[1:]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa7cca4d-85f6-440e-93a4-85bccc9edda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1         $5,169 \n",
       "2         $3,470 \n",
       "3         $3,183 \n",
       "4         $4,370 \n",
       "5         $3,171 \n",
       "          ...    \n",
       "5996      $6,250 \n",
       "5997      $5,328 \n",
       "5998      $6,157 \n",
       "5999     $11,206 \n",
       "6000     $30,507 \n",
       "Name: Price, Length: 6000, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6658764-402e-4b9d-a744-1c39070d27a0",
   "metadata": {},
   "source": [
    "The next thing that happens is the `.replace()` method. Documentation on the replace method is [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html). However, documentation can be a little hard to read, so let's just play around with it and see if we can figure out what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "041c51a6-5ef2-43c3-a2e5-1dc41c720e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1         5169 \n",
       "2         3470 \n",
       "3         3183 \n",
       "4         4370 \n",
       "5         3171 \n",
       "         ...   \n",
       "5996      6250 \n",
       "5997      5328 \n",
       "5998      6157 \n",
       "5999     11206 \n",
       "6000     30507 \n",
       "Name: Price, Length: 6000, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Price'].replace(to_replace='[\\$,]', value='', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1add5f-2d9d-4497-bf94-b36584ce3e5d",
   "metadata": {},
   "source": [
    "Okay, if we just use the `.replace()` command with the same parameters it looks like it gives us back almost what we want. However, the `dtype` is still `object`, so we're not quite there. However, it can be unclear what part of those paramaters is necessary. Let's try taking some out and changing things to see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1e25b0-ce2c-4b78-8506-e6bc5c1c6e1e",
   "metadata": {},
   "source": [
    "Below, I remove the `regex=True` parameter. Just to pull out the relevant part of the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5f9a9e7-c30d-41aa-99c6-ae326df0ac67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1         $5,169 \n",
       "2         $3,470 \n",
       "3         $3,183 \n",
       "4         $4,370 \n",
       "5         $3,171 \n",
       "          ...    \n",
       "5996      $6,250 \n",
       "5997      $5,328 \n",
       "5998      $6,157 \n",
       "5999     $11,206 \n",
       "6000     $30,507 \n",
       "Name: Price, Length: 6000, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Price'].replace(to_replace='[\\$,]', value='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d256d10a-b0cc-431b-87fe-05990b2fa67a",
   "metadata": {},
   "source": [
    "Okay, that seemed to do nothing. Let's look at the documentation and see if we can figure out why:\n",
    "\n",
    "\n",
    "```\n",
    "regex: bool or same types as to_replace, default False\n",
    "\n",
    "Whether to interpret to_replace and/or value as regular expressions. If this is True then to_replace must be a string. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None.\n",
    "```\n",
    "\n",
    "Ah, this interprets the `to_replace` parameter as a \"regular expression.\" A regular expression is a particular way to match characters in strings. They can get very complicated, but they are also really useful when working with string data. We won't use regular expressions much in our class, but if you are interested in reading a little more, a nice introduction can be found [here](https://www.oreilly.com/content/an-introduction-to-regular-expressions/). I don't recommend you spend much time learning regular expressions at this point in your data science journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b92da3-4d6b-486e-ae09-15c86e61de74",
   "metadata": {},
   "source": [
    "Let's try to figure out just what the `to_replace` parameter does. Let's just try to replace the `$` symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bd7ecf9-f57a-4da6-b1d8-ca218c1bd8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1         $5,169 \n",
       "2         $3,470 \n",
       "3         $3,183 \n",
       "4         $4,370 \n",
       "5         $3,171 \n",
       "          ...    \n",
       "5996      $6,250 \n",
       "5997      $5,328 \n",
       "5998      $6,157 \n",
       "5999     $11,206 \n",
       "6000     $30,507 \n",
       "Name: Price, Length: 6000, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Price'].replace(to_replace='$', value='', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63946a7b-95a8-4f51-ae70-cbd2f36b4b43",
   "metadata": {},
   "source": [
    "That didn't work. Let's try it with the `\\` like it was originally written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f45a1ee-c05c-439a-8008-43f1a6746402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1         5,169 \n",
       "2         3,470 \n",
       "3         3,183 \n",
       "4         4,370 \n",
       "5         3,171 \n",
       "          ...   \n",
       "5996      6,250 \n",
       "5997      5,328 \n",
       "5998      6,157 \n",
       "5999     11,206 \n",
       "6000     30,507 \n",
       "Name: Price, Length: 6000, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Price'].replace(to_replace='\\$', value='', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245da6ef-c6a3-4afe-b3b0-e704bc237d43",
   "metadata": {},
   "source": [
    "Ah, that worked. So, it seems like when you write a `$` symbol in the regular expression, you have to have a `\\` in front of it.\n",
    "\n",
    "What about the comma?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "410f7198-6fb5-4d6a-90d9-4b83adb6d9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1         $5169 \n",
       "2         $3470 \n",
       "3         $3183 \n",
       "4         $4370 \n",
       "5         $3171 \n",
       "          ...   \n",
       "5996      $6250 \n",
       "5997      $5328 \n",
       "5998      $6157 \n",
       "5999     $11206 \n",
       "6000     $30507 \n",
       "Name: Price, Length: 6000, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Price'].replace(to_replace=',', value='', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a84af5-0865-45e4-96d6-d0b0ca6a8645",
   "metadata": {},
   "source": [
    "With the comma we don't need the `\\`. Alright, something to file away for later. Let's try putting both the `\\$` and the `,` in the `to_replace` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97859191-0992-4e8f-9325-7b8af94ddb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1         $5,169 \n",
       "2         $3,470 \n",
       "3         $3,183 \n",
       "4         $4,370 \n",
       "5         $3,171 \n",
       "          ...    \n",
       "5996      $6,250 \n",
       "5997      $5,328 \n",
       "5998      $6,157 \n",
       "5999     $11,206 \n",
       "6000     $30,507 \n",
       "Name: Price, Length: 6000, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Price'].replace(to_replace='\\$,', value='', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4424becb-6582-4a7e-a4da-59402bb29558",
   "metadata": {},
   "source": [
    "Didn't do anything. So, the only thing missing from the original statement is the brackets around the `\\$,`. Without the brackets, maybe we can guess that it is looking for strings like `$,` in the text to replace. Let's try that be replacing it with something we know is there like `\\$5`, which is in the first row of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8be3c81-8efc-481f-b51e-02aff555a5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1           ,169 \n",
       "2         $3,470 \n",
       "3         $3,183 \n",
       "4         $4,370 \n",
       "5         $3,171 \n",
       "          ...    \n",
       "5996      $6,250 \n",
       "5997        ,328 \n",
       "5998      $6,157 \n",
       "5999     $11,206 \n",
       "6000     $30,507 \n",
       "Name: Price, Length: 6000, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Price'].replace(to_replace='\\$5', value='', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064734e-4bb8-4f6e-b47f-2e878f3c6a19",
   "metadata": {},
   "source": [
    "Yes, it is matching exact strings. We really want to match (and then replace) either a `$` **or** a `,`. That must be what the `[]` means. It means find either of those things and match and replace them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a6bb11e-8564-4e36-a9e3-9ad403444e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1         5169 \n",
       "2         3470 \n",
       "3         3183 \n",
       "4         4370 \n",
       "5         3171 \n",
       "         ...   \n",
       "5996      6250 \n",
       "5997      5328 \n",
       "5998      6157 \n",
       "5999     11206 \n",
       "6000     30507 \n",
       "Name: Price, Length: 6000, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Price'].replace(to_replace='[\\$,]', value='', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d983e-8200-48db-b3df-4fdf9e66b9db",
   "metadata": {},
   "source": [
    "Indeed. What if we add a `5` to the things we put in the brackets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "108abcc8-c687-4556-a12d-20c32c736506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1          169 \n",
       "2         3470 \n",
       "3         3183 \n",
       "4         4370 \n",
       "5         3171 \n",
       "         ...   \n",
       "5996       620 \n",
       "5997       328 \n",
       "5998       617 \n",
       "5999     11206 \n",
       "6000      3007 \n",
       "Name: Price, Length: 6000, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Price'].replace(to_replace='[\\$,5]', value='', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d34afaa-c803-4999-b48b-1dd5ece833fc",
   "metadata": {},
   "source": [
    "We get rid of all of the `5`s. We obviously don't want that, but now we have a sense for what is going on. What if we replace `value=''` with something else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d383a562-151e-4f67-a473-7cf7015742e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1         *5*169 \n",
       "2         *3*470 \n",
       "3         *3*183 \n",
       "4         *4*370 \n",
       "5         *3*171 \n",
       "          ...    \n",
       "5996      *6*250 \n",
       "5997      *5*328 \n",
       "5998      *6*157 \n",
       "5999     *11*206 \n",
       "6000     *30*507 \n",
       "Name: Price, Length: 6000, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Price'].replace(to_replace='[\\$,]', value='*', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cd1fe-5d14-4e4c-a088-e7db83dcfb9c",
   "metadata": {},
   "source": [
    "Ah, `value` is what is replaced, and when we set it to `value=''`, it says replace whatever you find with nothing, which effectively gets rid of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db18163b-acf8-4288-b5f6-f47a2c2c84e4",
   "metadata": {},
   "source": [
    "The final step is just to figure out what `.astype(float)` means. Hopefully, this step is a little easier. We can see that if we don't include it, the `dtype` of the data ends up being `object`. If we do include it, it becomes `float64`. So, it's just a way of telling the computer that the data should be viewed as a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ee7bd6f-0706-47d9-9401-3ad2b282608f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1        5169.0\n",
       "2        3470.0\n",
       "3        3183.0\n",
       "4        4370.0\n",
       "5        3171.0\n",
       "         ...   \n",
       "5996     6250.0\n",
       "5997     5328.0\n",
       "5998     6157.0\n",
       "5999    11206.0\n",
       "6000    30507.0\n",
       "Name: Price, Length: 6000, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Price'].replace(to_replace='[\\$,]', value='', regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108125d6-0ab9-4a4f-a791-00eed7d73f1f",
   "metadata": {},
   "source": [
    "We've worked through this example not because this is a very important example in and of itself, but it is very common for a programmer to find code somewhere online and have to figure out how and why it works. The process of exploring/playing with code is very helpful in figuring out what something does. Don't be afraid to add a new cell at any point in a notebook (use the `+` button in the toolbar at the top of the notebook) and experiment with changing up the code. This is the most effective way to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e3c339-133d-4ab1-935b-741df7d4d44a",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65da274-ecc5-432c-98ed-5f7f7bef0332",
   "metadata": {},
   "source": [
    "Okay, now we actually need to add this new column of price data that is correctly labeled as a number. To do that, we create a new column called `Price_numeric` and we just say it's equal to our transformed original column. We can do that as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8b7f68a-d417-463e-bd4b-4f931e2a0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Price_numeric'] = df_train['Price'].replace(to_replace='[\\$,]', value='', regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd5f98f-908a-49e4-b5a5-918faa5092d8",
   "metadata": {},
   "source": [
    "Let's make sure it stuck and look at our data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42fca152-0a61-4f94-a182-19b737f53024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>Unique Values</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Carat Weight</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>6000.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.33452</td>\n",
       "      <td>0.475696</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.59</td>\n",
       "      <td>2.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cut</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>2482</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6</td>\n",
       "      <td>G</td>\n",
       "      <td>1501</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clarity</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>7</td>\n",
       "      <td>SI1</td>\n",
       "      <td>2059</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polish</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>EX</td>\n",
       "      <td>2425</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Symmetry</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>VG</td>\n",
       "      <td>2417</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Report</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>GIA</td>\n",
       "      <td>5266</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>4821</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>4821</td>\n",
       "      <td>$4,466</td>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Price_numeric</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>4821</td>\n",
       "      <td>6000.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>11791.579333</td>\n",
       "      <td>10184.350051</td>\n",
       "      <td>2184.0</td>\n",
       "      <td>5150.5</td>\n",
       "      <td>7857.0</td>\n",
       "      <td>15036.5</td>\n",
       "      <td>101561.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Data Type  Missing Values  Unique Values   count unique  \\\n",
       "Carat Weight    float64               0            196  6000.0          \n",
       "Cut              object               0              5  6000.0      5   \n",
       "Color            object               0              6  6000.0      6   \n",
       "Clarity          object               0              7  6000.0      7   \n",
       "Polish           object               0              4  6000.0      4   \n",
       "Symmetry         object               0              4  6000.0      4   \n",
       "Report           object               0              2  6000.0      2   \n",
       "Price            object               0           4821  6000.0   4821   \n",
       "Price_numeric   float64               0           4821  6000.0          \n",
       "\n",
       "                    top  freq          mean           std     min     25%  \\\n",
       "Carat Weight                        1.33452      0.475696    0.75     1.0   \n",
       "Cut               Ideal  2482                                               \n",
       "Color                 G  1501                                               \n",
       "Clarity             SI1  2059                                               \n",
       "Polish               EX  2425                                               \n",
       "Symmetry             VG  2417                                               \n",
       "Report              GIA  5266                                               \n",
       "Price           $4,466      8                                               \n",
       "Price_numeric                  11791.579333  10184.350051  2184.0  5150.5   \n",
       "\n",
       "                  50%      75%       max  \n",
       "Carat Weight     1.13     1.59      2.91  \n",
       "Cut                                       \n",
       "Color                                     \n",
       "Clarity                                   \n",
       "Polish                                    \n",
       "Symmetry                                  \n",
       "Report                                    \n",
       "Price                                     \n",
       "Price_numeric  7857.0  15036.5  101561.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarize_dataframe(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60086aeb-b854-48fb-963c-11b4dbf5dfcc",
   "metadata": {},
   "source": [
    "It worked fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c51b2c-fef3-4caf-9c6d-db085d2cd182",
   "metadata": {},
   "source": [
    "Let's take a look a what is in one of our other columns of data. Let's say the `Cut` data. We know from the `summarize_dataframe` command that it consists of five unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ab45b76-9de6-4f6b-b734-1f3fe0f5d117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1                 Ideal\n",
       "2                 Ideal\n",
       "3                 Ideal\n",
       "4                 Ideal\n",
       "5                 Ideal\n",
       "             ...       \n",
       "5996              Ideal\n",
       "5997          Very Good\n",
       "5998              Ideal\n",
       "5999    Signature-Ideal\n",
       "6000              Ideal\n",
       "Name: Cut, Length: 6000, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Cut']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b996d-9d25-4680-bfb9-b9c4f852da2e",
   "metadata": {},
   "source": [
    "That tells us something, but it is a bit hard to figure out what the possible values are. We can get those by using the `.value_counts()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a514852-6b04-476e-842d-668f7b94e2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cut\n",
       "Ideal              2482\n",
       "Very Good          2428\n",
       "Good                708\n",
       "Signature-Ideal     253\n",
       "Fair                129\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Cut'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37290ae8-0b8f-4aca-914d-29e6cd8755aa",
   "metadata": {},
   "source": [
    "This tells us what all of the possible types are, and it gives use the number of each type. As we can see, `Ideal` is the most common cut followed by `Very Good`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646cd2b7-3306-483c-8716-7d1f02350b24",
   "metadata": {},
   "source": [
    "Feel free to explore the data some more here to get familiar with what is in it. I've left a blank cell to encourge you to do that, but you can always add your own cells with the plus button on the toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60552df1-53f0-497b-b333-323c6d31b44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3125d3d-dd9a-4acc-96d9-fcb886ed9372",
   "metadata": {},
   "source": [
    "## Split into `smaller_train` and `validation` Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915329d9-8ae5-4d5f-b957-4ba0a08d62be",
   "metadata": {},
   "source": [
    "However, we are not done yet manipulating our data. A best practice in machine learning is to further split up the training set into a smaller training set and a validation set. You can compare the performance of candidate models (each trained on the smaller training set) on the validation set. This `df_smaller_train` data set becomes our laboratory to test out different modeling decisions and try them out on the `df_validation` data set.\n",
    "\n",
    "This way of thinking about intentionally training with less data is not intuitive if you haven't been exposed to it. Read [this](https://machinelearningmastery.com/difference-test-validation-datasets/) article to to build a better understanding of why we do this.\n",
    "\n",
    "The following code randomly splits your training set into a smaller training set (75% of the training data) and a validation set (25% of the training data). There is no \"correct\" size for the smaller training set and the validation set. Here we have chosen a 75%-25% split, but we could chose something else. The tradeoff is that the more data you use for your smaller training set, the better the model, but you are less certain about it's performance due to having less validation data. The more validation data you have, the more confident you are in the model, but the less data you have to use to train. So, we will use a 75%-25% split, which is a fairly common split, but this is not a magic number.\n",
    "\n",
    "Additionally, the split is random. I.e., it doesn't take the first 75% of the rows and use them for the smaller training set, it takes any row with a 75% chance. This is very important to avoid biases in how your data might be organized. For example, you may have your data sorted by carat weight. If you did, you would end up with your smaller training set having all of the large carat weight examples and none of the small carat weight rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d20620c8-62c6-44cf-bd8f-2c15e1628e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smaller_train, df_validation = train_test_split(df_train, test_size=.25, random_state=201)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25482bf-adc5-4563-b0b0-4b495aee6e1f",
   "metadata": {},
   "source": [
    "The exact command `train_test_split` is imported from `sklearn.model_selection` in our import statement. We give it the data set `df_train`. We also tell it the `test_size` which is how much of our data we want devoted to the validation set (set to `.25` or 25%).\n",
    "\n",
    "We also include the parameter `random_state=201` in the function call. This is because the function `train_test_split` randomly splits your data. This is good practice. However, if we let it randomly split our data, everytime we run our code something different will happen (i.e. we end up with different random `df_smaller_train` and `df_validation` sets). We pass an initial _seed_ to the randomness generator in the function which makes the function give us _the same random split_ each time. This helps us with reproducibility with the code, and it is generally good practice when building models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf397ed-35e6-46f4-969d-d0c2389da443",
   "metadata": {},
   "source": [
    "Let's take a look at what we have in our `df_smaller_train` data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70a8a460-1505-4973-aa1c-9eca7dc5e923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>Unique Values</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Carat Weight</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>4500.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.327182</td>\n",
       "      <td>0.472008</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cut</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>1866</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>6</td>\n",
       "      <td>G</td>\n",
       "      <td>1135</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clarity</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>7</td>\n",
       "      <td>SI1</td>\n",
       "      <td>1568</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polish</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>EX</td>\n",
       "      <td>1813</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Symmetry</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>VG</td>\n",
       "      <td>1790</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Report</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>GIA</td>\n",
       "      <td>3933</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>3773</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>3773</td>\n",
       "      <td>$4,466</td>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Price_numeric</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>3773</td>\n",
       "      <td>4500.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>11647.470667</td>\n",
       "      <td>9987.135937</td>\n",
       "      <td>2184.0</td>\n",
       "      <td>5122.0</td>\n",
       "      <td>7795.0</td>\n",
       "      <td>14703.25</td>\n",
       "      <td>96493.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Data Type  Missing Values  Unique Values   count unique  \\\n",
       "Carat Weight    float64               0            187  4500.0          \n",
       "Cut              object               0              5  4500.0      5   \n",
       "Color            object               0              6  4500.0      6   \n",
       "Clarity          object               0              7  4500.0      7   \n",
       "Polish           object               0              4  4500.0      4   \n",
       "Symmetry         object               0              4  4500.0      4   \n",
       "Report           object               0              2  4500.0      2   \n",
       "Price            object               0           3773  4500.0   3773   \n",
       "Price_numeric   float64               0           3773  4500.0          \n",
       "\n",
       "                    top  freq          mean          std     min     25%  \\\n",
       "Carat Weight                       1.327182     0.472008    0.75     1.0   \n",
       "Cut               Ideal  1866                                              \n",
       "Color                 G  1135                                              \n",
       "Clarity             SI1  1568                                              \n",
       "Polish               EX  1813                                              \n",
       "Symmetry             VG  1790                                              \n",
       "Report              GIA  3933                                              \n",
       "Price           $4,466      6                                              \n",
       "Price_numeric                  11647.470667  9987.135937  2184.0  5122.0   \n",
       "\n",
       "                  50%       75%      max  \n",
       "Carat Weight     1.12      1.58     2.91  \n",
       "Cut                                       \n",
       "Color                                     \n",
       "Clarity                                   \n",
       "Polish                                    \n",
       "Symmetry                                  \n",
       "Report                                    \n",
       "Price                                     \n",
       "Price_numeric  7795.0  14703.25  96493.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarize_dataframe(df_smaller_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98390f6-8ddd-4a87-a9a2-211ba001b6d3",
   "metadata": {},
   "source": [
    "Indeed, we have `4500` rows, which is 75% of our original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c277b-5822-48c8-b661-4d874005973d",
   "metadata": {},
   "source": [
    "The validation data set is 25% of the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28eb5cef-20c8-4a37-855e-89533ea2a747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>Unique Values</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Carat Weight</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>1500.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.356533</td>\n",
       "      <td>0.486089</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cut</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>616</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>6</td>\n",
       "      <td>G</td>\n",
       "      <td>366</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clarity</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>7</td>\n",
       "      <td>SI1</td>\n",
       "      <td>491</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polish</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>EX</td>\n",
       "      <td>612</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Symmetry</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>VG</td>\n",
       "      <td>627</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Report</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>GIA</td>\n",
       "      <td>1333</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>1407</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1407</td>\n",
       "      <td>$4,771</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Price_numeric</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>1407</td>\n",
       "      <td>1500.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>12223.905333</td>\n",
       "      <td>10746.181569</td>\n",
       "      <td>2348.0</td>\n",
       "      <td>5227.0</td>\n",
       "      <td>7991.0</td>\n",
       "      <td>16177.0</td>\n",
       "      <td>101561.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Data Type  Missing Values  Unique Values   count unique  \\\n",
       "Carat Weight    float64               0            165  1500.0          \n",
       "Cut              object               0              5  1500.0      5   \n",
       "Color            object               0              6  1500.0      6   \n",
       "Clarity          object               0              7  1500.0      7   \n",
       "Polish           object               0              4  1500.0      4   \n",
       "Symmetry         object               0              4  1500.0      4   \n",
       "Report           object               0              2  1500.0      2   \n",
       "Price            object               0           1407  1500.0   1407   \n",
       "Price_numeric   float64               0           1407  1500.0          \n",
       "\n",
       "                    top  freq          mean           std     min     25%  \\\n",
       "Carat Weight                       1.356533      0.486089    0.75    1.01   \n",
       "Cut               Ideal   616                                               \n",
       "Color                 G   366                                               \n",
       "Clarity             SI1   491                                               \n",
       "Polish               EX   612                                               \n",
       "Symmetry             VG   627                                               \n",
       "Report              GIA  1333                                               \n",
       "Price           $4,771      4                                               \n",
       "Price_numeric                  12223.905333  10746.181569  2348.0  5227.0   \n",
       "\n",
       "                  50%      75%       max  \n",
       "Carat Weight     1.15     1.65      2.81  \n",
       "Cut                                       \n",
       "Color                                     \n",
       "Clarity                                   \n",
       "Polish                                    \n",
       "Symmetry                                  \n",
       "Report                                    \n",
       "Price                                     \n",
       "Price_numeric  7991.0  16177.0  101561.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarize_dataframe(df_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46115f0b-0ec2-4d88-afb8-ba9578ebc17b",
   "metadata": {},
   "source": [
    "Now, we can use our `df_smaller_train` to build our model and our `df_validation` to confirm the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764b459-a7f4-4a11-a050-b6305fe379a4",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6694d32-f3ad-496c-a34f-6428ccb4f6e9",
   "metadata": {},
   "source": [
    "We need some way to determine how well our model is performing. There are lots of ways to do this. However, in this particular exercise, we are going to be using the **mean absolute error (or MAE)**. Read [this](https://www.dataquest.io/blog/understanding-regression-error-metrics/) primer on various metrics, but pay particular attention to the mean absolute error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e39419-eb23-431a-ab32-3b832541dfa8",
   "metadata": {},
   "source": [
    "We will measure mean absolute error with the `mean_absolute_error()` function imported from `sklearn.metrics`. We will see how to do this after we generate some predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1263a22-98e1-4665-84eb-d92805a8c549",
   "metadata": {},
   "source": [
    "## Advanced Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca1462-4391-46e3-a5ec-6354b4450277",
   "metadata": {},
   "source": [
    "Now, we fit an actual regression model. For this exercise, we are going to be using the statsmodels module. Specifically, we are going to be using a submodule called the `statsmodels.formula.api` module which we have imported as `smf`. This allows us to write down regression formulas using language that looks nearly right out of a stats textbook. We are going to walk through the regressions from the case first and see how we would do them in python.\n",
    "\n",
    "```python\n",
    "lm_1 = smf.ols(formula='Price_numeric ~ Q(\"Carat Weight\")', data=df_smaller_train).fit()\n",
    "```\n",
    "\n",
    "We are creating a regression model and saving it in the variable `lm_1`. We are doing an ordinary least squares (or _ols_) regression, so we are using the function `smf.ols()`. We tell it the data we want to use with `data=df_smaller_train`. We define the regression model using the `formula` parameter.\n",
    "\n",
    "The formula has the \"dependent variable\" on the left hand side, with the equation divided by `~`, and then the \"independent variables\" are on the right hand side. To reference columns of our dataframe, we just use the column names. So our dependent variable is `Price_numeric` and our independent variable is `Carat Weight`.\n",
    "\n",
    "There is one wrinkle here. All of the variables in the formula need to be valid python variables. Unfortunately, a python variable cannot have a space in it. `Carat Weight`, however, has a space in it. So we have to wrap it up in the _quote_ function `Q()` and put some quotation marks around it to keep from getting an error.\n",
    "\n",
    "We will be using this formula notation throughout the class, and you shouldn't expect to master it the first time you see it. As you use it and as you want to go deeper, a great resource to master this formula syntax is the [_patsy_ documentation](https://patsy.readthedocs.io/en/latest/quickstart.html). `patsy` is the package that underlies this formula syntax, and while the examples in the link are not quite what we are doing here, they are close enough to be a helpful resource.\n",
    "\n",
    "Finally, the `.fit()` method actually trains the model on the data. Let's see it work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb146b67-1235-45b1-b82d-78af46d85437",
   "metadata": {},
   "source": [
    "### Additive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb2eec9-c309-4c7a-b939-f3c54b2f7e7d",
   "metadata": {},
   "source": [
    "Let's see how we would train the additive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "789b312e-16d6-476a-b272-6399e5cb817f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>Price_numeric</td>  <th>  R-squared:         </th> <td>   0.738</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.738</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.267e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 26 Oct 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:36:33</td>     <th>  Log-Likelihood:    </th> <td> -44811.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  4500</td>      <th>  AIC:               </th> <td>8.963e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  4498</td>      <th>  BIC:               </th> <td>8.964e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>         <td>-1.248e+04</td> <td>  227.441</td> <td>  -54.861</td> <td> 0.000</td> <td>-1.29e+04</td> <td> -1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Carat Weight\")</th> <td> 1.818e+04</td> <td>  161.466</td> <td>  112.579</td> <td> 0.000</td> <td> 1.79e+04</td> <td> 1.85e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>3668.307</td> <th>  Durbin-Watson:     </th>  <td>   2.008</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>152719.640</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 3.602</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>30.615</td>  <th>  Cond. No.          </th>  <td>    6.16</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &  Price\\_numeric  & \\textbf{  R-squared:         } &     0.738   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.738   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } & 1.267e+04   \\\\\n",
       "\\textbf{Date:}             & Thu, 26 Oct 2023 & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
       "\\textbf{Time:}             &     21:36:33     & \\textbf{  Log-Likelihood:    } &   -44811.   \\\\\n",
       "\\textbf{No. Observations:} &        4500      & \\textbf{  AIC:               } & 8.963e+04   \\\\\n",
       "\\textbf{Df Residuals:}     &        4498      & \\textbf{  BIC:               } & 8.964e+04   \\\\\n",
       "\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                           & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}         &   -1.248e+04  &      227.441     &   -54.861  &         0.000        &    -1.29e+04    &     -1.2e+04     \\\\\n",
       "\\textbf{Q(\"Carat Weight\")} &    1.818e+04  &      161.466     &   112.579  &         0.000        &     1.79e+04    &     1.85e+04     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 3668.307 & \\textbf{  Durbin-Watson:     } &     2.008   \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000  & \\textbf{  Jarque-Bera (JB):  } & 152719.640  \\\\\n",
       "\\textbf{Skew:}          &   3.602  & \\textbf{  Prob(JB):          } &      0.00   \\\\\n",
       "\\textbf{Kurtosis:}      &  30.615  & \\textbf{  Cond. No.          } &      6.16   \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:          Price_numeric   R-squared:                       0.738\n",
       "Model:                            OLS   Adj. R-squared:                  0.738\n",
       "Method:                 Least Squares   F-statistic:                 1.267e+04\n",
       "Date:                Thu, 26 Oct 2023   Prob (F-statistic):               0.00\n",
       "Time:                        21:36:33   Log-Likelihood:                -44811.\n",
       "No. Observations:                4500   AIC:                         8.963e+04\n",
       "Df Residuals:                    4498   BIC:                         8.964e+04\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "Intercept         -1.248e+04    227.441    -54.861      0.000   -1.29e+04    -1.2e+04\n",
       "Q(\"Carat Weight\")  1.818e+04    161.466    112.579      0.000    1.79e+04    1.85e+04\n",
       "==============================================================================\n",
       "Omnibus:                     3668.307   Durbin-Watson:                   2.008\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           152719.640\n",
       "Skew:                           3.602   Prob(JB):                         0.00\n",
       "Kurtosis:                      30.615   Cond. No.                         6.16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_1 = smf.ols(formula='Price_numeric ~ Q(\"Carat Weight\")', data=df_smaller_train).fit()\n",
    "lm_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606cdc2e-fa95-4404-b509-eaea109bab13",
   "metadata": {},
   "source": [
    "Above, the `.summary()` method for our regression model tells us what is in the regression model. Here we see that the intercept has a coefficient of `-1.248e+04` (this is in scientific notation, so it would 12,480 in common notation) and the `Carat Weight` has a coefficient of 18,180. This is very close to the model in the case. It's not identical because we are using a randomly selected sample of 75% of the data, so there is some variation from using the whole data set. We can also see the p values and other statistical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7040c7cd-f010-4d6e-be4d-6ef40b9efe2e",
   "metadata": {},
   "source": [
    "We can now use this model to predict on our `df_validation` data. We do this with the `.predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3912ebad-ed1d-4b27-840a-707003c764bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_1_predictions = lm_1.predict(df_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3455a2f4-e683-4732-bd7f-6d6434475a1c",
   "metadata": {},
   "source": [
    "Let's take a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "405453f3-1e43-4d9e-8c78-8f5f6af5ded8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1646    23877.733991\n",
       "4429     7881.379434\n",
       "3354     1519.192963\n",
       "2608    14607.119418\n",
       "3998     3882.290795\n",
       "            ...     \n",
       "4923     5881.835115\n",
       "947      7336.049165\n",
       "2152     4064.067551\n",
       "4438     1519.192963\n",
       "959      6245.388627\n",
       "Length: 1500, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_1_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a908f-acec-455e-998d-ef99906623ff",
   "metadata": {},
   "source": [
    "We can also measure our MAE for `lm_1` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c773edf6-5a84-42b5-8c4e-652f38783ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3095.795318100367"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(df_validation[\"Price_numeric\"], lm_1_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eaeba6-8edd-42c1-9fb4-2d47f5d7b69c",
   "metadata": {},
   "source": [
    "This means that, on average, we miss the price of a diamond by about $3000. Hopefully we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f66923-583b-45a3-99aa-daf2c8ced5c7",
   "metadata": {},
   "source": [
    "### Multiplicative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21664848-c6d1-42ce-a43d-b80598f8c2d6",
   "metadata": {},
   "source": [
    "Let's try the multiplicative model from the case. To do this, we just apply the function `np.log()`, a function that computes the natural logarithm (or _ln_ in normal math notation) to the dependent variable. It's very simple to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "052bcfe1-4062-4098-a351-164d370716a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>np.log(Price_numeric)</td> <th>  R-squared:         </th> <td>   0.845</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                     <td>OLS</td>          <th>  Adj. R-squared:    </th> <td>   0.845</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>Least Squares</td>     <th>  F-statistic:       </th> <td>2.449e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>               <td>Thu, 26 Oct 2023</td>    <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                   <td>21:36:34</td>        <th>  Log-Likelihood:    </th> <td> -623.68</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>        <td>  4500</td>         <th>  AIC:               </th> <td>   1251.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>            <td>  4498</td>         <th>  BIC:               </th> <td>   1264.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>                <td>     1</td>         <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>        <td>nonrobust</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>         <td>    7.2674</td> <td>    0.012</td> <td>  587.551</td> <td> 0.000</td> <td>    7.243</td> <td>    7.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Carat Weight\")</th> <td>    1.3743</td> <td>    0.009</td> <td>  156.505</td> <td> 0.000</td> <td>    1.357</td> <td>    1.391</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>285.145</td> <th>  Durbin-Watson:     </th> <td>   2.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 372.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.584</td>  <th>  Prob(JB):          </th> <td>1.45e-81</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.788</td>  <th>  Cond. No.          </th> <td>    6.16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    & np.log(Price\\_numeric) & \\textbf{  R-squared:         } &     0.845   \\\\\n",
       "\\textbf{Model:}            &          OLS           & \\textbf{  Adj. R-squared:    } &     0.845   \\\\\n",
       "\\textbf{Method:}           &     Least Squares      & \\textbf{  F-statistic:       } & 2.449e+04   \\\\\n",
       "\\textbf{Date:}             &    Thu, 26 Oct 2023    & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
       "\\textbf{Time:}             &        21:36:34        & \\textbf{  Log-Likelihood:    } &   -623.68   \\\\\n",
       "\\textbf{No. Observations:} &           4500         & \\textbf{  AIC:               } &     1251.   \\\\\n",
       "\\textbf{Df Residuals:}     &           4498         & \\textbf{  BIC:               } &     1264.   \\\\\n",
       "\\textbf{Df Model:}         &              1         & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &       nonrobust        & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                           & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}         &       7.2674  &        0.012     &   587.551  &         0.000        &        7.243    &        7.292     \\\\\n",
       "\\textbf{Q(\"Carat Weight\")} &       1.3743  &        0.009     &   156.505  &         0.000        &        1.357    &        1.391     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 285.145 & \\textbf{  Durbin-Watson:     } &    2.052  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &  372.276  \\\\\n",
       "\\textbf{Skew:}          &   0.584 & \\textbf{  Prob(JB):          } & 1.45e-81  \\\\\n",
       "\\textbf{Kurtosis:}      &   3.788 & \\textbf{  Cond. No.          } &     6.16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                              OLS Regression Results                             \n",
       "=================================================================================\n",
       "Dep. Variable:     np.log(Price_numeric)   R-squared:                       0.845\n",
       "Model:                               OLS   Adj. R-squared:                  0.845\n",
       "Method:                    Least Squares   F-statistic:                 2.449e+04\n",
       "Date:                   Thu, 26 Oct 2023   Prob (F-statistic):               0.00\n",
       "Time:                           21:36:34   Log-Likelihood:                -623.68\n",
       "No. Observations:                   4500   AIC:                             1251.\n",
       "Df Residuals:                       4498   BIC:                             1264.\n",
       "Df Model:                              1                                         \n",
       "Covariance Type:               nonrobust                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "Intercept             7.2674      0.012    587.551      0.000       7.243       7.292\n",
       "Q(\"Carat Weight\")     1.3743      0.009    156.505      0.000       1.357       1.391\n",
       "==============================================================================\n",
       "Omnibus:                      285.145   Durbin-Watson:                   2.052\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              372.276\n",
       "Skew:                           0.584   Prob(JB):                     1.45e-81\n",
       "Kurtosis:                       3.788   Cond. No.                         6.16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_2 = smf.ols(formula='np.log(Price_numeric) ~ Q(\"Carat Weight\")', data=df_smaller_train).fit()\n",
    "lm_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0be2242-2b91-4e6f-8568-e9ccd6b3d640",
   "metadata": {},
   "source": [
    "Again, we get coefficients very similar to what is observed in the case. We can also predict on our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "696bc657-3a8a-4d9a-bea0-a33bc6ab7709",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_2_predictions = lm_2.predict(df_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee5250-ddf0-4845-a9df-afc678234e75",
   "metadata": {},
   "source": [
    "Let's take a look at our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3672ccf2-b727-48fe-b500-8a0ebe7cb76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1646    10.015898\n",
       "4429     8.806545\n",
       "3354     8.325552\n",
       "2608     9.315023\n",
       "3998     8.504206\n",
       "          ...    \n",
       "4923     8.655375\n",
       "947      8.765317\n",
       "2152     8.517949\n",
       "4438     8.325552\n",
       "959      8.682861\n",
       "Length: 1500, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_2_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056737c8-1bfd-4401-9405-0a6ea30ccd5d",
   "metadata": {},
   "source": [
    "There seems to be a problem with our predictions. It appears like we are predicting that the price of a diamond is $8. There must be something else going on.\n",
    "\n",
    "Remember, that we are predicting the natural logarithm of the price (`np.log(Price_numeric)`), not the price itself. So, we need to convert the predicted log prices back to true prices. We can do this with the `np.exp()` function. This is the exponential function and the exponential function \"reverses\" the log function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75e3bd52-4d7e-4d57-a7e8-0c5708f6a77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1646    22379.432403\n",
       "4429     6677.804378\n",
       "3354     4128.014497\n",
       "2608    11103.576378\n",
       "3998     4935.485141\n",
       "            ...     \n",
       "4923     5740.923644\n",
       "947      6408.090299\n",
       "2152     5003.779981\n",
       "4438     4128.014497\n",
       "959      5900.903109\n",
       "Length: 1500, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(lm_2_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e810ab6f-4ff2-4b76-a322-9c7b91523f28",
   "metadata": {},
   "source": [
    "That looks a bit better. Let's see how it does for MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a6b5dfb-8115-4563-85b8-e2e9b5e2c7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2941.669739657505"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(df_validation[\"Price_numeric\"], np.exp(lm_2_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6915d106-a16e-4e61-898e-dca5876951cc",
   "metadata": {},
   "source": [
    "Our MAE has gone down! So, we are improving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2aff55-1924-421d-ac88-983f6a754acc",
   "metadata": {},
   "source": [
    "### Log-Log Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeabf09-0962-47e7-9cf9-a0e041204de6",
   "metadata": {},
   "source": [
    "Let's do the log-log model from the case. Again, this is simple. We just use the `np.log` function, but on the independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20ab54bb-3897-4938-9283-4da4463a66b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>np.log(Price_numeric)</td> <th>  R-squared:         </th> <td>   0.866</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                     <td>OLS</td>          <th>  Adj. R-squared:    </th> <td>   0.866</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>Least Squares</td>     <th>  F-statistic:       </th> <td>2.916e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>               <td>Thu, 26 Oct 2023</td>    <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                   <td>21:36:34</td>        <th>  Log-Likelihood:    </th> <td> -287.82</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>        <td>  4500</td>         <th>  AIC:               </th> <td>   579.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>            <td>  4498</td>         <th>  BIC:               </th> <td>   592.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>                <td>     1</td>         <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>        <td>nonrobust</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "              <td></td>                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                 <td>    8.6418</td> <td>    0.005</td> <td> 1854.200</td> <td> 0.000</td> <td>    8.633</td> <td>    8.651</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.log(Q(\"Carat Weight\"))</th> <td>    1.9893</td> <td>    0.012</td> <td>  170.766</td> <td> 0.000</td> <td>    1.966</td> <td>    2.012</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>459.461</td> <th>  Durbin-Watson:     </th> <td>   2.032</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 674.271</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.777</td>  <th>  Prob(JB):          </th> <td>3.84e-147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.086</td>  <th>  Cond. No.          </th> <td>    3.20</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}            & np.log(Price\\_numeric) & \\textbf{  R-squared:         } &     0.866   \\\\\n",
       "\\textbf{Model:}                    &          OLS           & \\textbf{  Adj. R-squared:    } &     0.866   \\\\\n",
       "\\textbf{Method:}                   &     Least Squares      & \\textbf{  F-statistic:       } & 2.916e+04   \\\\\n",
       "\\textbf{Date:}                     &    Thu, 26 Oct 2023    & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
       "\\textbf{Time:}                     &        21:36:34        & \\textbf{  Log-Likelihood:    } &   -287.82   \\\\\n",
       "\\textbf{No. Observations:}         &           4500         & \\textbf{  AIC:               } &     579.6   \\\\\n",
       "\\textbf{Df Residuals:}             &           4498         & \\textbf{  BIC:               } &     592.5   \\\\\n",
       "\\textbf{Df Model:}                 &              1         & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}          &       nonrobust        & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                 &       8.6418  &        0.005     &  1854.200  &         0.000        &        8.633    &        8.651     \\\\\n",
       "\\textbf{np.log(Q(\"Carat Weight\"))} &       1.9893  &        0.012     &   170.766  &         0.000        &        1.966    &        2.012     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 459.461 & \\textbf{  Durbin-Watson:     } &     2.032  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &   674.271  \\\\\n",
       "\\textbf{Skew:}          &   0.777 & \\textbf{  Prob(JB):          } & 3.84e-147  \\\\\n",
       "\\textbf{Kurtosis:}      &   4.086 & \\textbf{  Cond. No.          } &      3.20  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                              OLS Regression Results                             \n",
       "=================================================================================\n",
       "Dep. Variable:     np.log(Price_numeric)   R-squared:                       0.866\n",
       "Model:                               OLS   Adj. R-squared:                  0.866\n",
       "Method:                    Least Squares   F-statistic:                 2.916e+04\n",
       "Date:                   Thu, 26 Oct 2023   Prob (F-statistic):               0.00\n",
       "Time:                           21:36:34   Log-Likelihood:                -287.82\n",
       "No. Observations:                   4500   AIC:                             579.6\n",
       "Df Residuals:                       4498   BIC:                             592.5\n",
       "Df Model:                              1                                         \n",
       "Covariance Type:               nonrobust                                         \n",
       "=============================================================================================\n",
       "                                coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------\n",
       "Intercept                     8.6418      0.005   1854.200      0.000       8.633       8.651\n",
       "np.log(Q(\"Carat Weight\"))     1.9893      0.012    170.766      0.000       1.966       2.012\n",
       "==============================================================================\n",
       "Omnibus:                      459.461   Durbin-Watson:                   2.032\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              674.271\n",
       "Skew:                           0.777   Prob(JB):                    3.84e-147\n",
       "Kurtosis:                       4.086   Cond. No.                         3.20\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_3 = smf.ols(formula='np.log(Price_numeric) ~ np.log(Q(\"Carat Weight\"))', data=df_smaller_train).fit()\n",
    "lm_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146bb1da-c89e-4535-aba3-8c30a7fd55f1",
   "metadata": {},
   "source": [
    "Again, we get coefficients very similar to what is observed in the case. We can also predict on our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cee80b13-19a5-4004-b610-b49d2a3f6b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_3_predictions = lm_3.predict(df_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1d885-760f-48f4-ad71-671d11c17b8c",
   "metadata": {},
   "source": [
    "Let's take a look at our predictions. Again, we need to \"reverse\" the log function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c78b378-1c5f-4cfc-a14c-e671afccfb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1646    22486.680736\n",
       "4429     7095.571053\n",
       "3354     3367.196680\n",
       "2608    12519.919628\n",
       "3998     4592.502279\n",
       "            ...     \n",
       "4923     5776.608625\n",
       "947      6722.488652\n",
       "2152     4694.571513\n",
       "4438     3367.196680\n",
       "959      6006.393931\n",
       "Length: 1500, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(lm_3_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5167746e-894b-40b2-b8cd-8b1625f884c6",
   "metadata": {},
   "source": [
    "That looks a bit better. Let's see how it does for MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b95635a-cea2-49b8-b1b2-b1e5f2579e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2796.5795921112262"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(df_validation[\"Price_numeric\"], np.exp(lm_3_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f040a692-8c5d-42b1-a70f-901ffffb6459",
   "metadata": {},
   "source": [
    "Our MAE has gone down again!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f06341-a888-487a-877a-0ddd1fdbccb3",
   "metadata": {},
   "source": [
    "### Models with Multiple Independent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9230b4bc-54ad-4267-9f2c-c90464d852c5",
   "metadata": {},
   "source": [
    "If we want to add more variables, we can just put them in. Let's add `Cut`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a249d949-2f52-4982-8289-1d88c6372325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>np.log(Price_numeric)</td> <th>  R-squared:         </th> <td>   0.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                     <td>OLS</td>          <th>  Adj. R-squared:    </th> <td>   0.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>Least Squares</td>     <th>  F-statistic:       </th> <td>   6338.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>               <td>Thu, 26 Oct 2023</td>    <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                   <td>21:36:34</td>        <th>  Log-Likelihood:    </th> <td> -122.97</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>        <td>  4500</td>         <th>  AIC:               </th> <td>   257.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>            <td>  4494</td>         <th>  BIC:               </th> <td>   296.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>                <td>     5</td>         <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>        <td>nonrobust</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "              <td></td>                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                 <td>    8.4978</td> <td>    0.026</td> <td>  325.730</td> <td> 0.000</td> <td>    8.447</td> <td>    8.549</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Cut[T.Good]</th>               <td>    0.0405</td> <td>    0.028</td> <td>    1.434</td> <td> 0.152</td> <td>   -0.015</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Cut[T.Ideal]</th>              <td>    0.1885</td> <td>    0.027</td> <td>    7.027</td> <td> 0.000</td> <td>    0.136</td> <td>    0.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Cut[T.Signature-Ideal]</th>    <td>    0.3560</td> <td>    0.032</td> <td>   11.258</td> <td> 0.000</td> <td>    0.294</td> <td>    0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Cut[T.Very Good]</th>          <td>    0.1184</td> <td>    0.027</td> <td>    4.416</td> <td> 0.000</td> <td>    0.066</td> <td>    0.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.log(Q(\"Carat Weight\"))</th> <td>    1.9796</td> <td>    0.011</td> <td>  174.908</td> <td> 0.000</td> <td>    1.957</td> <td>    2.002</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>465.639</td> <th>  Durbin-Watson:     </th> <td>   2.028</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 710.885</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.767</td>  <th>  Prob(JB):          </th> <td>4.30e-155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.200</td>  <th>  Cond. No.          </th> <td>    19.1</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}            & np.log(Price\\_numeric) & \\textbf{  R-squared:         } &     0.876   \\\\\n",
       "\\textbf{Model:}                    &          OLS           & \\textbf{  Adj. R-squared:    } &     0.876   \\\\\n",
       "\\textbf{Method:}                   &     Least Squares      & \\textbf{  F-statistic:       } &     6338.   \\\\\n",
       "\\textbf{Date:}                     &    Thu, 26 Oct 2023    & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
       "\\textbf{Time:}                     &        21:36:34        & \\textbf{  Log-Likelihood:    } &   -122.97   \\\\\n",
       "\\textbf{No. Observations:}         &           4500         & \\textbf{  AIC:               } &     257.9   \\\\\n",
       "\\textbf{Df Residuals:}             &           4494         & \\textbf{  BIC:               } &     296.4   \\\\\n",
       "\\textbf{Df Model:}                 &              5         & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}          &       nonrobust        & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                 &       8.4978  &        0.026     &   325.730  &         0.000        &        8.447    &        8.549     \\\\\n",
       "\\textbf{Cut[T.Good]}               &       0.0405  &        0.028     &     1.434  &         0.152        &       -0.015    &        0.096     \\\\\n",
       "\\textbf{Cut[T.Ideal]}              &       0.1885  &        0.027     &     7.027  &         0.000        &        0.136    &        0.241     \\\\\n",
       "\\textbf{Cut[T.Signature-Ideal]}    &       0.3560  &        0.032     &    11.258  &         0.000        &        0.294    &        0.418     \\\\\n",
       "\\textbf{Cut[T.Very Good]}          &       0.1184  &        0.027     &     4.416  &         0.000        &        0.066    &        0.171     \\\\\n",
       "\\textbf{np.log(Q(\"Carat Weight\"))} &       1.9796  &        0.011     &   174.908  &         0.000        &        1.957    &        2.002     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 465.639 & \\textbf{  Durbin-Watson:     } &     2.028  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &   710.885  \\\\\n",
       "\\textbf{Skew:}          &   0.767 & \\textbf{  Prob(JB):          } & 4.30e-155  \\\\\n",
       "\\textbf{Kurtosis:}      &   4.200 & \\textbf{  Cond. No.          } &      19.1  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                              OLS Regression Results                             \n",
       "=================================================================================\n",
       "Dep. Variable:     np.log(Price_numeric)   R-squared:                       0.876\n",
       "Model:                               OLS   Adj. R-squared:                  0.876\n",
       "Method:                    Least Squares   F-statistic:                     6338.\n",
       "Date:                   Thu, 26 Oct 2023   Prob (F-statistic):               0.00\n",
       "Time:                           21:36:34   Log-Likelihood:                -122.97\n",
       "No. Observations:                   4500   AIC:                             257.9\n",
       "Df Residuals:                       4494   BIC:                             296.4\n",
       "Df Model:                              5                                         \n",
       "Covariance Type:               nonrobust                                         \n",
       "=============================================================================================\n",
       "                                coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------\n",
       "Intercept                     8.4978      0.026    325.730      0.000       8.447       8.549\n",
       "Cut[T.Good]                   0.0405      0.028      1.434      0.152      -0.015       0.096\n",
       "Cut[T.Ideal]                  0.1885      0.027      7.027      0.000       0.136       0.241\n",
       "Cut[T.Signature-Ideal]        0.3560      0.032     11.258      0.000       0.294       0.418\n",
       "Cut[T.Very Good]              0.1184      0.027      4.416      0.000       0.066       0.171\n",
       "np.log(Q(\"Carat Weight\"))     1.9796      0.011    174.908      0.000       1.957       2.002\n",
       "==============================================================================\n",
       "Omnibus:                      465.639   Durbin-Watson:                   2.028\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              710.885\n",
       "Skew:                           0.767   Prob(JB):                    4.30e-155\n",
       "Kurtosis:                       4.200   Cond. No.                         19.1\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_4 = smf.ols(formula='np.log(Price_numeric) ~ Cut + np.log(Q(\"Carat Weight\"))', data=df_smaller_train).fit()\n",
    "lm_4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c10fd0-cec5-4c4b-84e8-3cfc441ae095",
   "metadata": {},
   "source": [
    "If you look at the coefficients, you see that we don't have a single `Cut` variable like we have for `np.log(Q(\"Carat Weight\"))`. Instead, we end up with a number of variables like `Cut[T.Good]`, `Cut[T.Ideal]`, `Cut[T.Signature-Ideal]`, and `Cut[T.Very Good]`. Let's figure out what that is about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b23e8b-1215-4e1f-bd9a-0c27d9330e29",
   "metadata": {},
   "source": [
    "Let's look at the possible values for `Cut`. We did this above in the notebook, but let's do it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8d48549-d9b0-4ae2-8b4a-fc79096aeea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cut\n",
       "Ideal              1866\n",
       "Very Good          1817\n",
       "Good                531\n",
       "Signature-Ideal     195\n",
       "Fair                 91\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_smaller_train['Cut'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3878b4c7-8a3c-48eb-b433-0e8671122e31",
   "metadata": {},
   "source": [
    "So, each of our variables corresponds to one of the possible values for `Cut`, but we don't have a variable for `Fair`. What is going on here is that the regression model `smf.ols()` knows that the `Cut` data is categorical (because the `dtype` is `object`, as we saw above), and it automatically creates dummy variables for each category. It also drops one of the categories (generally the least common class) because you are supposed to drop one dummy column to avoid overspecifciation in a linear model. So, the coefficients correspond to dummy variables for each category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c284d797-2ece-48b5-b3b8-b3238d8cfd90",
   "metadata": {},
   "source": [
    "Let's see how this model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "666bd97c-8881-4dab-a912-b2d030816e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_4_predictions = lm_4.predict(df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78286e12-3f7a-460a-a634-eb7f9173aa3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2709.1194824784543"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(df_validation[\"Price_numeric\"], np.exp(lm_4_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509151d3-203f-4e39-ac4c-7db4c3f77dd5",
   "metadata": {},
   "source": [
    "Even better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2a133f-a3ed-423d-82ea-0566f2abd07e",
   "metadata": {},
   "source": [
    "### Model with Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e126fa9e-0c50-4b10-a434-690ab3531697",
   "metadata": {},
   "source": [
    "We can also easily interact two variables. Suppose that we think that `Cut` becomes more important with a higher `Carat Weight`. We can interact these two variables as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d1fc89b6-302d-446c-9358-66904261213b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>np.log(Price_numeric)</td> <th>  R-squared:         </th> <td>   0.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                     <td>OLS</td>          <th>  Adj. R-squared:    </th> <td>   0.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>Least Squares</td>     <th>  F-statistic:       </th> <td>   3520.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>               <td>Thu, 26 Oct 2023</td>    <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                   <td>21:36:35</td>        <th>  Log-Likelihood:    </th> <td> -122.03</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>        <td>  4500</td>         <th>  AIC:               </th> <td>   264.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>            <td>  4490</td>         <th>  BIC:               </th> <td>   328.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>                <td>     9</td>         <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>        <td>nonrobust</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                          <td></td>                            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                        <td>    8.5002</td> <td>    0.026</td> <td>  321.342</td> <td> 0.000</td> <td>    8.448</td> <td>    8.552</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Cut[T.Good]</th>                                      <td>    0.0445</td> <td>    0.029</td> <td>    1.520</td> <td> 0.129</td> <td>   -0.013</td> <td>    0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Cut[T.Ideal]</th>                                     <td>    0.1856</td> <td>    0.027</td> <td>    6.765</td> <td> 0.000</td> <td>    0.132</td> <td>    0.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Cut[T.Signature-Ideal]</th>                           <td>    0.3463</td> <td>    0.034</td> <td>   10.234</td> <td> 0.000</td> <td>    0.280</td> <td>    0.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Cut[T.Very Good]</th>                                 <td>    0.1148</td> <td>    0.027</td> <td>    4.194</td> <td> 0.000</td> <td>    0.061</td> <td>    0.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.log(Q(\"Carat Weight\"))</th>                        <td>    1.9075</td> <td>    0.128</td> <td>   14.896</td> <td> 0.000</td> <td>    1.656</td> <td>    2.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Cut[T.Good]:np.log(Q(\"Carat Weight\"))</th>            <td>    0.0367</td> <td>    0.133</td> <td>    0.277</td> <td> 0.782</td> <td>   -0.223</td> <td>    0.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Cut[T.Ideal]:np.log(Q(\"Carat Weight\"))</th>           <td>    0.0742</td> <td>    0.129</td> <td>    0.574</td> <td> 0.566</td> <td>   -0.179</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Cut[T.Signature-Ideal]:np.log(Q(\"Carat Weight\"))</th> <td>    0.1181</td> <td>    0.147</td> <td>    0.804</td> <td> 0.422</td> <td>   -0.170</td> <td>    0.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Cut[T.Very Good]:np.log(Q(\"Carat Weight\"))</th>       <td>    0.0775</td> <td>    0.129</td> <td>    0.600</td> <td> 0.549</td> <td>   -0.176</td> <td>    0.331</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>463.763</td> <th>  Durbin-Watson:     </th> <td>   2.029</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 707.260</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.765</td>  <th>  Prob(JB):          </th> <td>2.63e-154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.197</td>  <th>  Cond. No.          </th> <td>    93.7</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                                   & np.log(Price\\_numeric) & \\textbf{  R-squared:         } &     0.876   \\\\\n",
       "\\textbf{Model:}                                           &          OLS           & \\textbf{  Adj. R-squared:    } &     0.876   \\\\\n",
       "\\textbf{Method:}                                          &     Least Squares      & \\textbf{  F-statistic:       } &     3520.   \\\\\n",
       "\\textbf{Date:}                                            &    Thu, 26 Oct 2023    & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
       "\\textbf{Time:}                                            &        21:36:35        & \\textbf{  Log-Likelihood:    } &   -122.03   \\\\\n",
       "\\textbf{No. Observations:}                                &           4500         & \\textbf{  AIC:               } &     264.1   \\\\\n",
       "\\textbf{Df Residuals:}                                    &           4490         & \\textbf{  BIC:               } &     328.2   \\\\\n",
       "\\textbf{Df Model:}                                        &              9         & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                                 &       nonrobust        & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                        &       8.5002  &        0.026     &   321.342  &         0.000        &        8.448    &        8.552     \\\\\n",
       "\\textbf{Cut[T.Good]}                                      &       0.0445  &        0.029     &     1.520  &         0.129        &       -0.013    &        0.102     \\\\\n",
       "\\textbf{Cut[T.Ideal]}                                     &       0.1856  &        0.027     &     6.765  &         0.000        &        0.132    &        0.239     \\\\\n",
       "\\textbf{Cut[T.Signature-Ideal]}                           &       0.3463  &        0.034     &    10.234  &         0.000        &        0.280    &        0.413     \\\\\n",
       "\\textbf{Cut[T.Very Good]}                                 &       0.1148  &        0.027     &     4.194  &         0.000        &        0.061    &        0.168     \\\\\n",
       "\\textbf{np.log(Q(\"Carat Weight\"))}                        &       1.9075  &        0.128     &    14.896  &         0.000        &        1.656    &        2.158     \\\\\n",
       "\\textbf{Cut[T.Good]:np.log(Q(\"Carat Weight\"))}            &       0.0367  &        0.133     &     0.277  &         0.782        &       -0.223    &        0.297     \\\\\n",
       "\\textbf{Cut[T.Ideal]:np.log(Q(\"Carat Weight\"))}           &       0.0742  &        0.129     &     0.574  &         0.566        &       -0.179    &        0.327     \\\\\n",
       "\\textbf{Cut[T.Signature-Ideal]:np.log(Q(\"Carat Weight\"))} &       0.1181  &        0.147     &     0.804  &         0.422        &       -0.170    &        0.406     \\\\\n",
       "\\textbf{Cut[T.Very Good]:np.log(Q(\"Carat Weight\"))}       &       0.0775  &        0.129     &     0.600  &         0.549        &       -0.176    &        0.331     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 463.763 & \\textbf{  Durbin-Watson:     } &     2.029  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &   707.260  \\\\\n",
       "\\textbf{Skew:}          &   0.765 & \\textbf{  Prob(JB):          } & 2.63e-154  \\\\\n",
       "\\textbf{Kurtosis:}      &   4.197 & \\textbf{  Cond. No.          } &      93.7  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                              OLS Regression Results                             \n",
       "=================================================================================\n",
       "Dep. Variable:     np.log(Price_numeric)   R-squared:                       0.876\n",
       "Model:                               OLS   Adj. R-squared:                  0.876\n",
       "Method:                    Least Squares   F-statistic:                     3520.\n",
       "Date:                   Thu, 26 Oct 2023   Prob (F-statistic):               0.00\n",
       "Time:                           21:36:35   Log-Likelihood:                -122.03\n",
       "No. Observations:                   4500   AIC:                             264.1\n",
       "Df Residuals:                       4490   BIC:                             328.2\n",
       "Df Model:                              9                                         \n",
       "Covariance Type:               nonrobust                                         \n",
       "====================================================================================================================\n",
       "                                                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                            8.5002      0.026    321.342      0.000       8.448       8.552\n",
       "Cut[T.Good]                                          0.0445      0.029      1.520      0.129      -0.013       0.102\n",
       "Cut[T.Ideal]                                         0.1856      0.027      6.765      0.000       0.132       0.239\n",
       "Cut[T.Signature-Ideal]                               0.3463      0.034     10.234      0.000       0.280       0.413\n",
       "Cut[T.Very Good]                                     0.1148      0.027      4.194      0.000       0.061       0.168\n",
       "np.log(Q(\"Carat Weight\"))                            1.9075      0.128     14.896      0.000       1.656       2.158\n",
       "Cut[T.Good]:np.log(Q(\"Carat Weight\"))                0.0367      0.133      0.277      0.782      -0.223       0.297\n",
       "Cut[T.Ideal]:np.log(Q(\"Carat Weight\"))               0.0742      0.129      0.574      0.566      -0.179       0.327\n",
       "Cut[T.Signature-Ideal]:np.log(Q(\"Carat Weight\"))     0.1181      0.147      0.804      0.422      -0.170       0.406\n",
       "Cut[T.Very Good]:np.log(Q(\"Carat Weight\"))           0.0775      0.129      0.600      0.549      -0.176       0.331\n",
       "==============================================================================\n",
       "Omnibus:                      463.763   Durbin-Watson:                   2.029\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              707.260\n",
       "Skew:                           0.765   Prob(JB):                    2.63e-154\n",
       "Kurtosis:                       4.197   Cond. No.                         93.7\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_5 = smf.ols(formula='np.log(Price_numeric) ~ Cut + np.log(Q(\"Carat Weight\")) + Cut*np.log(Q(\"Carat Weight\"))', data=df_smaller_train).fit()\n",
    "lm_5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60234a54-d46a-423e-b627-b1d00e70bbdb",
   "metadata": {},
   "source": [
    "The model automatically creates interactions between each category for `Cut` and `np.log(Q(\"Carat Weight\"))`. We can then test the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cba46e1b-b351-4569-bc4f-e4853b327d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_5_predictions = lm_5.predict(df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "757f76d2-34b5-4849-abf8-316d70d2f560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2706.122723169632"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(df_validation[\"Price_numeric\"], np.exp(lm_5_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e2438d-c58f-4e71-b5a3-0edf28717ac2",
   "metadata": {},
   "source": [
    "That helped us, but only a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d255af4-4fc1-4cb2-a5b3-ffed398c9902",
   "metadata": {},
   "source": [
    "### ADVANCED: Segmenting Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2080ec30-6ac9-484b-a9dd-9f5ba45bad45",
   "metadata": {},
   "source": [
    "This is an advanced section, and you are not expected to follow everything in this section. This is provided both as a resource as you progress to more advanced topics to come back to, and as a way for those who want to dive deep to go a little deeper. Feel free to skim through it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb63cf-4dd9-4787-b1bc-7289710e5860",
   "metadata": {},
   "source": [
    "The formula specification is very flexible. Suppose that we think the relationship between `Carat Weight` and `ln(Price_numeric)` changes for different ranges of `Carat Weight`. Suppose that we think there is one relationship that holds from weights 0 to 1, a different one that holds from 1 to 2, and a third for anything above 2. We can implement that using a linear spline as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8539132f-c3e5-40d0-be6b-138ebe9a9cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>np.log(Price_numeric)</td> <th>  R-squared:         </th> <td>   0.870</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                     <td>OLS</td>          <th>  Adj. R-squared:    </th> <td>   0.869</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>Least Squares</td>     <th>  F-statistic:       </th> <td>   9991.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>               <td>Thu, 26 Oct 2023</td>    <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                   <td>21:36:35</td>        <th>  Log-Likelihood:    </th> <td> -233.30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>        <td>  4500</td>         <th>  AIC:               </th> <td>   474.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>            <td>  4496</td>         <th>  BIC:               </th> <td>   500.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>                <td>     3</td>         <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>        <td>nonrobust</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                    <td></td>                      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                            <td>    5.7503</td> <td>    0.061</td> <td>   94.597</td> <td> 0.000</td> <td>    5.631</td> <td>    5.870</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Carat Weight\")</th>                    <td>    2.9668</td> <td>    0.064</td> <td>   46.459</td> <td> 0.000</td> <td>    2.842</td> <td>    3.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.maximum(Q(\"Carat Weight\") - 1, 0)</th> <td>   -1.6280</td> <td>    0.069</td> <td>  -23.512</td> <td> 0.000</td> <td>   -1.764</td> <td>   -1.492</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.maximum(Q(\"Carat Weight\") - 2, 0)</th> <td>   -0.5884</td> <td>    0.054</td> <td>  -10.944</td> <td> 0.000</td> <td>   -0.694</td> <td>   -0.483</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>425.364</td> <th>  Durbin-Watson:     </th> <td>   2.039</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 606.515</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.745</td>  <th>  Prob(JB):          </th> <td>1.98e-132</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.009</td>  <th>  Cond. No.          </th> <td>    52.1</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                       & np.log(Price\\_numeric) & \\textbf{  R-squared:         } &     0.870   \\\\\n",
       "\\textbf{Model:}                               &          OLS           & \\textbf{  Adj. R-squared:    } &     0.869   \\\\\n",
       "\\textbf{Method:}                              &     Least Squares      & \\textbf{  F-statistic:       } &     9991.   \\\\\n",
       "\\textbf{Date:}                                &    Thu, 26 Oct 2023    & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
       "\\textbf{Time:}                                &        21:36:35        & \\textbf{  Log-Likelihood:    } &   -233.30   \\\\\n",
       "\\textbf{No. Observations:}                    &           4500         & \\textbf{  AIC:               } &     474.6   \\\\\n",
       "\\textbf{Df Residuals:}                        &           4496         & \\textbf{  BIC:               } &     500.2   \\\\\n",
       "\\textbf{Df Model:}                            &              3         & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                     &       nonrobust        & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                              & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                            &       5.7503  &        0.061     &    94.597  &         0.000        &        5.631    &        5.870     \\\\\n",
       "\\textbf{Q(\"Carat Weight\")}                    &       2.9668  &        0.064     &    46.459  &         0.000        &        2.842    &        3.092     \\\\\n",
       "\\textbf{np.maximum(Q(\"Carat Weight\") - 1, 0)} &      -1.6280  &        0.069     &   -23.512  &         0.000        &       -1.764    &       -1.492     \\\\\n",
       "\\textbf{np.maximum(Q(\"Carat Weight\") - 2, 0)} &      -0.5884  &        0.054     &   -10.944  &         0.000        &       -0.694    &       -0.483     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 425.364 & \\textbf{  Durbin-Watson:     } &     2.039  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &   606.515  \\\\\n",
       "\\textbf{Skew:}          &   0.745 & \\textbf{  Prob(JB):          } & 1.98e-132  \\\\\n",
       "\\textbf{Kurtosis:}      &   4.009 & \\textbf{  Cond. No.          } &      52.1  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                              OLS Regression Results                             \n",
       "=================================================================================\n",
       "Dep. Variable:     np.log(Price_numeric)   R-squared:                       0.870\n",
       "Model:                               OLS   Adj. R-squared:                  0.869\n",
       "Method:                    Least Squares   F-statistic:                     9991.\n",
       "Date:                   Thu, 26 Oct 2023   Prob (F-statistic):               0.00\n",
       "Time:                           21:36:35   Log-Likelihood:                -233.30\n",
       "No. Observations:                   4500   AIC:                             474.6\n",
       "Df Residuals:                       4496   BIC:                             500.2\n",
       "Df Model:                              3                                         \n",
       "Covariance Type:               nonrobust                                         \n",
       "========================================================================================================\n",
       "                                           coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------------------------------\n",
       "Intercept                                5.7503      0.061     94.597      0.000       5.631       5.870\n",
       "Q(\"Carat Weight\")                        2.9668      0.064     46.459      0.000       2.842       3.092\n",
       "np.maximum(Q(\"Carat Weight\") - 1, 0)    -1.6280      0.069    -23.512      0.000      -1.764      -1.492\n",
       "np.maximum(Q(\"Carat Weight\") - 2, 0)    -0.5884      0.054    -10.944      0.000      -0.694      -0.483\n",
       "==============================================================================\n",
       "Omnibus:                      425.364   Durbin-Watson:                   2.039\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              606.515\n",
       "Skew:                           0.745   Prob(JB):                    1.98e-132\n",
       "Kurtosis:                       4.009   Cond. No.                         52.1\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_6 = smf.ols(formula='np.log(Price_numeric) ~ Q(\"Carat Weight\") + np.maximum(Q(\"Carat Weight\") - 1, 0) + np.maximum(Q(\"Carat Weight\") - 2, 0)', data=df_smaller_train).fit()\n",
    "lm_6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3279d83-67cb-4988-aef7-cd80d82dc6f6",
   "metadata": {},
   "source": [
    "Above, we effectively have different linear relationships for our three different ranges. Let's see if it helped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33515751-4716-4b74-8739-790be57453f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_6_predictions = lm_6.predict(df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "860b354b-469b-4afc-baba-a12e481f4d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2787.9615596509575"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(df_validation[\"Price_numeric\"], np.exp(lm_6_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f30c373-0a4d-4759-9e7d-2e0d0134c3d5",
   "metadata": {},
   "source": [
    "It's not as good as the best model we've found so far (that is `lm_5`), but it is significantly better than the multiplicative model (`lm_2`) which is the closest model to this one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157c9b4-0507-4cd7-b8d5-7f2a3a3595a8",
   "metadata": {},
   "source": [
    "## Your Turn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baf27bb-f68d-486e-9ce9-59542dcb36ee",
   "metadata": {},
   "source": [
    "Now that you have seen how to train linear regression models, it is your turn to see how good of a model you can build. You should mix and match all of the above techniques to build a model that performs as well as possible on the validation set. Feel free to experiment. You are likely to run into errors, try your best to figure out what the error means. It is often helpful to copy and paste the _last line_ of the error into google and see what comes up.\n",
    "\n",
    "You should copy and paste code from above into the notebook. You can hold shift and left click on cell in order to select multiple cells. Then you can go to \"Edit\" in the toolbar, select \"Copy Cells\", move to another location in the notebook, and then select \"Paste Cells Below\" (or above if you so choose).\n",
    "\n",
    "Make sure that you test your models on the validation set. You should select the model with the lowest MAE on the validation set.\n",
    "\n",
    "I have added multiple empty cells below to encourage you to fill them, but you can always add empty cells by clicking on the \"plus\" icon on the toolbar. Go forth and model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba953ebb-af5e-4402-82b1-22e8139c56c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600.3999407474377"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_9 = smf.ols(formula='np.log(Price_numeric) ~ Q(\"Carat Weight\") + Cut + Clarity + Report + Color + Symmetry + Polish + Polish * Clarity + Polish * Cut + Polish * Symmetry + Polish * Color + Clarity * Color + Clarity * Cut + Clarity * Symmetry + Symmetry * Color + Symmetry * Cut + Color * Cut + np.log(Q(\"Carat Weight\")) + Symmetry*np.log(Q(\"Carat Weight\")) + Polish*np.log(Q(\"Carat Weight\")) + Color*np.log(Q(\"Carat Weight\")) + Cut*np.log(Q(\"Carat Weight\")) + np.maximum(Q(\"Carat Weight\") - 1, 0) + np.maximum(Q(\"Carat Weight\") - 2, 0) + Report*np.log(Q(\"Carat Weight\"))', data=df_train).fit()\n",
    "lm_9.summary()\n",
    "lm_9_predictions = lm_9.predict(df_validation)\n",
    "mean_absolute_error(df_validation[\"Price_numeric\"], np.exp(lm_9_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "adddb1eb-d03c-42e4-b26e-b6184943a580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_predictions = lm_9.predict(df_test)\n",
    "test_predictions.to_csv(\"DiamondSubmission.csv\", header=[\"Price Prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6695033-63f8-4e4c-8522-d703b91b0626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Read in both datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "submission_df = pd.read_csv('DiamondSubmission.csv')\n",
    "\n",
    "# 2. Merge based on the common identifier (e.g., 'ID')\n",
    "merged_df = pd.merge(train_df, submission_df, on='ID', how='outer')\n",
    "\n",
    "# Save the merged dataframe if needed\n",
    "merged_df.to_csv('merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "391e5b6e-43a0-4f83-abd9-b89fe6e113a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Statistics for train_df:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Carat Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3000.500000</td>\n",
       "      <td>1.334520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1732.195139</td>\n",
       "      <td>0.475696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1500.750000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3000.500000</td>\n",
       "      <td>1.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4500.250000</td>\n",
       "      <td>1.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6000.000000</td>\n",
       "      <td>2.910000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  Carat Weight\n",
       "count  6000.000000   6000.000000\n",
       "mean   3000.500000      1.334520\n",
       "std    1732.195139      0.475696\n",
       "min       1.000000      0.750000\n",
       "25%    1500.750000      1.000000\n",
       "50%    3000.500000      1.130000\n",
       "75%    4500.250000      1.590000\n",
       "max    6000.000000      2.910000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive Statistics for test_df:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Carat Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3142.000000</td>\n",
       "      <td>3142.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7571.500000</td>\n",
       "      <td>1.33923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>907.161599</td>\n",
       "      <td>0.47834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6001.000000</td>\n",
       "      <td>0.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6786.250000</td>\n",
       "      <td>1.01000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7571.500000</td>\n",
       "      <td>1.13000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8356.750000</td>\n",
       "      <td>1.61000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9142.000000</td>\n",
       "      <td>2.79000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  Carat Weight\n",
       "count  3142.000000    3142.00000\n",
       "mean   7571.500000       1.33923\n",
       "std     907.161599       0.47834\n",
       "min    6001.000000       0.75000\n",
       "25%    6786.250000       1.01000\n",
       "50%    7571.500000       1.13000\n",
       "75%    8356.750000       1.61000\n",
       "max    9142.000000       2.79000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Read in the datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Display the descriptive statistics for train_df\n",
    "print(\"Descriptive Statistics for train_df:\")\n",
    "display(train_df.describe())\n",
    "\n",
    "# Display the descriptive statistics for test_df\n",
    "print(\"\\nDescriptive Statistics for test_df:\")\n",
    "display(test_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bfab0964-5915-4d15-8ef0-b173d68f8b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "price_predictions_df = pd.read_csv('DiamondSubmission.csv')\n",
    "\n",
    "# Make sure the IDs align between test_df and price_predictions_df\n",
    "test_df = test_df.sort_values(by='ID')\n",
    "price_predictions_df = price_predictions_df.sort_values(by='ID')\n",
    "\n",
    "# Add the 'Price Predictions' to the test_df\n",
    "test_df['Price Prediction'] = price_predictions_df['Price Prediction'].values\n",
    "\n",
    "# Concatenate the train and test dataframes\n",
    "full_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Save the new dataframe with price predictions to a new CSV file\n",
    "full_df.to_csv('train_with_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f114c22c-2e87-4339-93af-414c1e45e8a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "598.6359068066952"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_9 = smf.ols(formula='np.log(Price_numeric) ~ Q(\"Carat Weight\") + Cut + Clarity + Report + Color + Symmetry + Polish + Report * Clarity + Report * Cut + Report * Polish + Report * Symmetry + Polish * Clarity + Polish * Cut + Polish * Symmetry + Polish * Color + Clarity * Color + Clarity * Cut + Clarity * Symmetry + Symmetry * Color + Symmetry * Cut + Color * Cut + np.log(Q(\"Carat Weight\")) + Symmetry*np.log(Q(\"Carat Weight\")) + Polish*np.log(Q(\"Carat Weight\")) + Color*np.log(Q(\"Carat Weight\")) + Cut*np.log(Q(\"Carat Weight\")) + np.maximum(Q(\"Carat Weight\") - 1, 0) + np.maximum(Q(\"Carat Weight\") - 2, 0) + Report*np.log(Q(\"Carat Weight\"))', data=df_train).fit()\n",
    "lm_9.summary()\n",
    "lm_9_predictions = lm_9.predict(df_validation)\n",
    "mean_absolute_error(df_validation[\"Price_numeric\"], np.exp(lm_9_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e18acd5b-a1e4-4d06-be27-b583199d983b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_predictions = lm_9.predict(df_test)\n",
    "test_predictions.to_csv(\"DiamondSubmission.csv\", header=[\"Price Prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f854678a-e9c2-400d-9d21-7c02393906b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Assuming you have the predictions in a Series or DataFrame called price_predictions_df\n",
    "# Ensure the predictions align with the test dataset rows\n",
    "assert len(test_df) == len(price_predictions_df), \"Mismatched row count between test data and predictions.\"\n",
    "\n",
    "# Add the predictions to the test dataset\n",
    "test_df['Price Prediction'] = price_predictions_df['Price Prediction'].values\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "test_df.to_csv('test_with_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "58084377-1882-46c2-b344-b40cff50dbe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Carat Weight</th>\n",
       "      <th>Cut</th>\n",
       "      <th>Color</th>\n",
       "      <th>Clarity</th>\n",
       "      <th>Polish</th>\n",
       "      <th>Symmetry</th>\n",
       "      <th>Report</th>\n",
       "      <th>Price</th>\n",
       "      <th>Price Prediction</th>\n",
       "      <th>Abs_Residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.10</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>VG</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "      <td>5169.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.83</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>H</td>\n",
       "      <td>VS1</td>\n",
       "      <td>ID</td>\n",
       "      <td>ID</td>\n",
       "      <td>AGSL</td>\n",
       "      <td>3470.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.85</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>EX</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "      <td>3183.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.91</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>VG</td>\n",
       "      <td>VG</td>\n",
       "      <td>GIA</td>\n",
       "      <td>4370.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.83</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>G</td>\n",
       "      <td>SI1</td>\n",
       "      <td>EX</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "      <td>3171.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Carat Weight    Cut Color Clarity Polish Symmetry Report   Price  \\\n",
       "0   1          1.10  Ideal     H     SI1     VG       EX    GIA  5169.0   \n",
       "1   2          0.83  Ideal     H     VS1     ID       ID   AGSL  3470.0   \n",
       "2   3          0.85  Ideal     H     SI1     EX       EX    GIA  3183.0   \n",
       "3   4          0.91  Ideal     E     SI1     VG       VG    GIA  4370.0   \n",
       "4   5          0.83  Ideal     G     SI1     EX       EX    GIA  3171.0   \n",
       "\n",
       "   Price Prediction  Abs_Residual  \n",
       "0               NaN           NaN  \n",
       "1               NaN           NaN  \n",
       "2               NaN           NaN  \n",
       "3               NaN           NaN  \n",
       "4               NaN           NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_df['Abs_Residual'] = abs(full_df['Price'] - full_df['Price Prediction'])\n",
    "\n",
    "sorted_diamonds = full_df.sort_values(by='Abs_Residual', ascending=False)\n",
    "\n",
    "top_candidates = sorted_diamonds.head(5)\n",
    "display(top_candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "574ba298-40b7-48d3-8e20-24fa18d48e21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Carat Weight</th>\n",
       "      <th>Cut</th>\n",
       "      <th>Color</th>\n",
       "      <th>Clarity</th>\n",
       "      <th>Polish</th>\n",
       "      <th>Symmetry</th>\n",
       "      <th>Report</th>\n",
       "      <th>Price</th>\n",
       "      <th>Price Prediction</th>\n",
       "      <th>Residual</th>\n",
       "      <th>Abs_Residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1.05</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>VG</td>\n",
       "      <td>G</td>\n",
       "      <td>GIA</td>\n",
       "      <td>$7,666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1.52</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>VS1</td>\n",
       "      <td>EX</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "      <td>$17,659</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1.28</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>I</td>\n",
       "      <td>VS1</td>\n",
       "      <td>EX</td>\n",
       "      <td>VG</td>\n",
       "      <td>GIA</td>\n",
       "      <td>$6,726</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>2.03</td>\n",
       "      <td>Good</td>\n",
       "      <td>G</td>\n",
       "      <td>VS1</td>\n",
       "      <td>VG</td>\n",
       "      <td>VG</td>\n",
       "      <td>GIA</td>\n",
       "      <td>$23,726</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>1.56</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>G</td>\n",
       "      <td>VS1</td>\n",
       "      <td>EX</td>\n",
       "      <td>EX</td>\n",
       "      <td>GIA</td>\n",
       "      <td>$14,957</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  Carat Weight        Cut Color Clarity Polish Symmetry Report  \\\n",
       "9   10          1.05  Very Good     E     VS1     VG        G    GIA   \n",
       "16  17          1.52      Ideal     D     VS1     EX       EX    GIA   \n",
       "17  18          1.28  Very Good     I     VS1     EX       VG    GIA   \n",
       "25  26          2.03       Good     G     VS1     VG       VG    GIA   \n",
       "33  34          1.56      Ideal     G     VS1     EX       EX    GIA   \n",
       "\n",
       "        Price  Price Prediction Residual Abs_Residual  \n",
       "9     $7,666                NaN      NaN          NaN  \n",
       "16   $17,659                NaN      NaN          NaN  \n",
       "17    $6,726                NaN      NaN          NaN  \n",
       "25   $23,726                NaN      NaN          NaN  \n",
       "33   $14,957                NaN      NaN          NaN  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df['Residual'] = full_df['Price'] - full_df['Price Prediction']\n",
    "sorted_df = full_df.sort_values(by='Residual')\n",
    "filtered_df = sorted_df[(sorted_df['Carat Weight'] >= 1) & (sorted_df['Clarity'] == 'VS1')]\n",
    "top_candidates = filtered_df.head(5)\n",
    "top_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d4d9d76b-b33f-405b-9b81-324ca121758e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID  Price Prediction\n",
      "7964  7965          7.689807\n",
      "8765  8766          7.691773\n",
      "7751  7752          7.700469\n",
      "6534  6535          7.750415\n",
      "8172  8173          7.787629\n",
      "        ID  Carat Weight        Cut Color Clarity Polish Symmetry Report  \\\n",
      "7964  7965          0.76  Very Good     I     SI1      G        G    GIA   \n",
      "8765  8766          0.75  Very Good     I     SI1     VG       VG    GIA   \n",
      "7751  7752          0.76  Very Good     I     SI1      G       VG    GIA   \n",
      "6534  6535          0.77       Fair     I     VS1      G       VG   AGSL   \n",
      "8172  8173          0.76  Very Good     I     VS2      G       EX    GIA   \n",
      "\n",
      "      Price  Price Prediction  \n",
      "7964    NaN          7.689807  \n",
      "8765    NaN          7.691773  \n",
      "7751    NaN          7.700469  \n",
      "6534    NaN          7.750415  \n",
      "8172    NaN          7.787629  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extracting the top 5 diamonds with the lowest predicted prices\n",
    "best_deals = full_df.nsmallest(5, 'Price Prediction')[['ID', 'Price Prediction']]\n",
    "\n",
    "print(best_deals)\n",
    "\n",
    "# Extracting the top 5 diamonds with the lowest predicted prices\n",
    "best_deals = full_df.nsmallest(5, 'Price Prediction')\n",
    "\n",
    "print(best_deals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "498e47a0-47e6-46e0-8498-5843978f990a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Price  Price Prediction  Residual\n",
      "6000    NaN          9.777460       NaN\n",
      "6001    NaN         10.714899       NaN\n",
      "6002    NaN          8.121892       NaN\n",
      "6003    NaN         10.111531       NaN\n",
      "6004    NaN          9.632726       NaN\n"
     ]
    }
   ],
   "source": [
    "full_df = pd.read_csv('train_with_predictions.csv')\n",
    "\n",
    "print(full_df['Price Prediction'].dropna().head(10))\n",
    "\n",
    "valid_predictions = full_df[~full_df['Price Prediction'].isna()]\n",
    "valid_predictions['Residual'] = valid_predictions['Price_numeric'] - valid_predictions['Price Prediction']\n",
    "\n",
    "top_candidates = valid_predictions.nlargest(5, 'Residual')\n",
    "print(top_candidates)\n",
    "\n",
    "# Convert the 'Price' column to a numeric format\n",
    "full_df['Price'] = full_df['Price'].str.replace(',', '').str.replace('$', '').astype(float)\n",
    "\n",
    "# Filter the dataframe for rows that have valid price predictions\n",
    "valid_predictions = full_df[~full_df['Price Prediction'].isna()].copy()  # Added copy() to address potential SettingWithCopyWarning\n",
    "\n",
    "# Compute the residuals\n",
    "valid_predictions['Residual'] = valid_predictions['Price'] - np.exp(valid_predictions['Price Prediction'])\n",
    "\n",
    "# Get the top 5 diamonds with the largest residuals\n",
    "top_candidates = valid_predictions.nlargest(5, 'Residual')\n",
    "\n",
    "# Display the top candidates\n",
    "print(top_candidates[['Price', 'Price Prediction', 'Residual']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d3115d29-5ef4-4413-8978-f0c3e8293dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "price_predictions_df = pd.read_csv('DiamondSubmission.csv')\n",
    "\n",
    "# Make sure the IDs align between test_df and price_predictions_df\n",
    "test_df = test_df.sort_values(by='ID')\n",
    "price_predictions_df = price_predictions_df.sort_values(by='ID')\n",
    "\n",
    "# Add the 'Price Predictions' to the test_df\n",
    "test_df['Price Prediction'] = price_predictions_df['Price Prediction'].values\n",
    "\n",
    "# Concatenate the train and test dataframes\n",
    "full_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Save the new dataframe with price predictions to a new CSV file\n",
    "full_df.to_csv('train_with_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "53fa9c18-48d6-4c79-836d-89460bf228f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              OLS Regression Results                             \n",
      "=================================================================================\n",
      "Dep. Variable:     np.log(Price_numeric)   R-squared:                       0.992\n",
      "Model:                               OLS   Adj. R-squared:                  0.992\n",
      "Method:                    Least Squares   F-statistic:                     3710.\n",
      "Date:                   Thu, 26 Oct 2023   Prob (F-statistic):               0.00\n",
      "Time:                           21:36:37   Log-Likelihood:                 7942.9\n",
      "No. Observations:                   6000   AIC:                        -1.551e+04\n",
      "Df Residuals:                       5810   BIC:                        -1.423e+04\n",
      "Df Model:                            189                                         \n",
      "Covariance Type:               nonrobust                                         \n",
      "====================================================================================================================\n",
      "                                                       coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                            6.4443      0.067     96.481      0.000       6.313       6.575\n",
      "Cut[T.Good]                                          0.0158      0.029      0.550      0.582      -0.041       0.072\n",
      "Cut[T.Ideal]                                         0.5394      0.045     12.075      0.000       0.452       0.627\n",
      "Cut[T.Signature-Ideal]                               0.7525      0.045     16.575      0.000       0.663       0.841\n",
      "Cut[T.Very Good]                                     0.0539      0.027      1.966      0.049       0.000       0.108\n",
      "Clarity[T.IF]                                        0.5182      0.053      9.784      0.000       0.414       0.622\n",
      "Clarity[T.SI1]                                      -0.4646      0.028    -16.468      0.000      -0.520      -0.409\n",
      "Clarity[T.VS1]                                      -0.2042      0.031     -6.588      0.000      -0.265      -0.143\n",
      "Clarity[T.VS2]                                      -0.3057      0.030    -10.343      0.000      -0.364      -0.248\n",
      "Clarity[T.VVS1]                                      0.1225      0.066      1.861      0.063      -0.007       0.252\n",
      "Clarity[T.VVS2]                                      0.0301      0.036      0.840      0.401      -0.040       0.100\n",
      "Report[T.GIA]                                        0.0370      0.007      5.040      0.000       0.023       0.051\n",
      "Color[T.E]                                          -0.0830      0.021     -3.888      0.000      -0.125      -0.041\n",
      "Color[T.F]                                          -0.1314      0.022     -5.896      0.000      -0.175      -0.088\n",
      "Color[T.G]                                          -0.2800      0.023    -12.063      0.000      -0.325      -0.234\n",
      "Color[T.H]                                          -0.4111      0.022    -18.347      0.000      -0.455      -0.367\n",
      "Color[T.I]                                          -0.4901      0.025    -19.886      0.000      -0.538      -0.442\n",
      "Symmetry[T.G]                                       -0.0524      0.021     -2.458      0.014      -0.094      -0.011\n",
      "Symmetry[T.ID]                                      -0.0515      0.048     -1.070      0.285      -0.146       0.043\n",
      "Symmetry[T.VG]                                      -0.0402      0.020     -1.990      0.047      -0.080      -0.001\n",
      "Polish[T.G]                                         -0.0515      0.024     -2.143      0.032      -0.099      -0.004\n",
      "Polish[T.ID]                                         0.0699      0.044      1.594      0.111      -0.016       0.156\n",
      "Polish[T.VG]                                        -0.0259      0.016     -1.594      0.111      -0.058       0.006\n",
      "Polish[T.G]:Clarity[T.IF]                           -0.0043      0.020     -0.221      0.825      -0.043       0.034\n",
      "Polish[T.ID]:Clarity[T.IF]                           0.0297      0.071      0.419      0.675      -0.109       0.169\n",
      "Polish[T.VG]:Clarity[T.IF]                           0.0080      0.010      0.779      0.436      -0.012       0.028\n",
      "Polish[T.G]:Clarity[T.SI1]                          -0.0126      0.008     -1.589      0.112      -0.028       0.003\n",
      "Polish[T.ID]:Clarity[T.SI1]                          0.0174      0.035      0.504      0.614      -0.050       0.085\n",
      "Polish[T.VG]:Clarity[T.SI1]                         -0.0113      0.005     -2.328      0.020      -0.021      -0.002\n",
      "Polish[T.G]:Clarity[T.VS1]                          -0.0114      0.010     -1.144      0.253      -0.031       0.008\n",
      "Polish[T.ID]:Clarity[T.VS1]                          0.0150      0.032      0.475      0.635      -0.047       0.077\n",
      "Polish[T.VG]:Clarity[T.VS1]                         -0.0040      0.006     -0.693      0.488      -0.015       0.007\n",
      "Polish[T.G]:Clarity[T.VS2]                          -0.0133      0.009     -1.500      0.134      -0.031       0.004\n",
      "Polish[T.ID]:Clarity[T.VS2]                         -0.0061      0.041     -0.150      0.881      -0.086       0.074\n",
      "Polish[T.VG]:Clarity[T.VS2]                         -0.0096      0.005     -1.839      0.066      -0.020       0.001\n",
      "Polish[T.G]:Clarity[T.VVS1]                          0.0057      0.022      0.252      0.801      -0.038       0.050\n",
      "Polish[T.ID]:Clarity[T.VVS1]                         0.0012      0.049      0.024      0.981      -0.095       0.098\n",
      "Polish[T.VG]:Clarity[T.VVS1]                        -0.0129      0.010     -1.268      0.205      -0.033       0.007\n",
      "Polish[T.G]:Clarity[T.VVS2]                         -0.0156      0.013     -1.228      0.219      -0.040       0.009\n",
      "Polish[T.ID]:Clarity[T.VVS2]                         0.0127      0.037      0.341      0.733      -0.060       0.085\n",
      "Polish[T.VG]:Clarity[T.VVS2]                         0.0040      0.007      0.592      0.554      -0.009       0.017\n",
      "Polish[T.G]:Cut[T.Good]                              0.0370      0.023      1.640      0.101      -0.007       0.081\n",
      "Polish[T.ID]:Cut[T.Good]                          8.275e-16   3.42e-16      2.419      0.016    1.57e-16     1.5e-15\n",
      "Polish[T.VG]:Cut[T.Good]                             0.0186      0.018      1.014      0.311      -0.017       0.054\n",
      "Polish[T.G]:Cut[T.Ideal]                            -0.0014      0.024     -0.059      0.953      -0.049       0.046\n",
      "Polish[T.ID]:Cut[T.Ideal]                            0.0522      0.051      1.021      0.308      -0.048       0.152\n",
      "Polish[T.VG]:Cut[T.Ideal]                           -0.0022      0.018     -0.125      0.900      -0.037       0.033\n",
      "Polish[T.G]:Cut[T.Signature-Ideal]               -7.526e-16   1.72e-16     -4.371      0.000   -1.09e-15   -4.15e-16\n",
      "Polish[T.ID]:Cut[T.Signature-Ideal]                  0.0126      0.012      1.054      0.292      -0.011       0.036\n",
      "Polish[T.VG]:Cut[T.Signature-Ideal]               1.521e-15   2.33e-16      6.532      0.000    1.06e-15    1.98e-15\n",
      "Polish[T.G]:Cut[T.Very Good]                         0.0109      0.022      0.500      0.617      -0.032       0.054\n",
      "Polish[T.ID]:Cut[T.Very Good]                        0.0051      0.026      0.193      0.847      -0.047       0.057\n",
      "Polish[T.VG]:Cut[T.Very Good]                        0.0109      0.018      0.619      0.536      -0.024       0.045\n",
      "Polish[T.G]:Symmetry[T.G]                            0.0354      0.015      2.353      0.019       0.006       0.065\n",
      "Polish[T.ID]:Symmetry[T.G]                       -5.939e-16   2.75e-16     -2.162      0.031   -1.13e-15   -5.55e-17\n",
      "Polish[T.VG]:Symmetry[T.G]                           0.0212      0.009      2.386      0.017       0.004       0.039\n",
      "Polish[T.G]:Symmetry[T.ID]                       -1.807e-15   3.33e-16     -5.431      0.000   -2.46e-15   -1.15e-15\n",
      "Polish[T.ID]:Symmetry[T.ID]                         -0.0123      0.041     -0.303      0.762      -0.092       0.068\n",
      "Polish[T.VG]:Symmetry[T.ID]                      -9.812e-16   2.88e-16     -3.403      0.001   -1.55e-15   -4.16e-16\n",
      "Polish[T.G]:Symmetry[T.VG]                           0.0250      0.013      1.855      0.064      -0.001       0.051\n",
      "Polish[T.ID]:Symmetry[T.VG]                      -1.458e-15   3.13e-16     -4.661      0.000   -2.07e-15   -8.45e-16\n",
      "Polish[T.VG]:Symmetry[T.VG]                          0.0229      0.005      4.628      0.000       0.013       0.033\n",
      "Polish[T.G]:Color[T.E]                              -0.0053      0.014     -0.375      0.707      -0.033       0.022\n",
      "Polish[T.ID]:Color[T.E]                             -0.1343      0.113     -1.193      0.233      -0.355       0.086\n",
      "Polish[T.VG]:Color[T.E]                             -0.0049      0.009     -0.552      0.581      -0.022       0.012\n",
      "Polish[T.G]:Color[T.F]                              -0.0200      0.013     -1.485      0.138      -0.046       0.006\n",
      "Polish[T.ID]:Color[T.F]                             -0.1105      0.063     -1.754      0.080      -0.234       0.013\n",
      "Polish[T.VG]:Color[T.F]                             -0.0180      0.008     -2.159      0.031      -0.034      -0.002\n",
      "Polish[T.G]:Color[T.G]                               0.0016      0.013      0.121      0.904      -0.024       0.027\n",
      "Polish[T.ID]:Color[T.G]                             -0.0826      0.066     -1.247      0.213      -0.212       0.047\n",
      "Polish[T.VG]:Color[T.G]                             -0.0035      0.008     -0.445      0.656      -0.019       0.012\n",
      "Polish[T.G]:Color[T.H]                               0.0022      0.014      0.158      0.874      -0.025       0.029\n",
      "Polish[T.ID]:Color[T.H]                             -0.0825      0.063     -1.304      0.192      -0.206       0.042\n",
      "Polish[T.VG]:Color[T.H]                             -0.0100      0.008     -1.198      0.231      -0.026       0.006\n",
      "Polish[T.G]:Color[T.I]                               0.0042      0.014      0.302      0.762      -0.023       0.032\n",
      "Polish[T.ID]:Color[T.I]                             -0.0512      0.068     -0.749      0.454      -0.185       0.083\n",
      "Polish[T.VG]:Color[T.I]                             -0.0037      0.009     -0.419      0.675      -0.021       0.014\n",
      "Clarity[T.IF]:Color[T.E]                            -0.1835      0.017    -10.602      0.000      -0.217      -0.150\n",
      "Clarity[T.SI1]:Color[T.E]                            0.0368      0.007      5.068      0.000       0.023       0.051\n",
      "Clarity[T.VS1]:Color[T.E]                            0.0315      0.010      3.279      0.001       0.013       0.050\n",
      "Clarity[T.VS2]:Color[T.E]                            0.0598      0.008      7.073      0.000       0.043       0.076\n",
      "Clarity[T.VVS1]:Color[T.E]                          -0.0079      0.017     -0.474      0.636      -0.041       0.025\n",
      "Clarity[T.VVS2]:Color[T.E]                          -0.0198      0.010     -1.886      0.059      -0.040       0.001\n",
      "Clarity[T.IF]:Color[T.F]                            -0.2268      0.014    -16.601      0.000      -0.254      -0.200\n",
      "Clarity[T.SI1]:Color[T.F]                            0.0535      0.007      7.674      0.000       0.040       0.067\n",
      "Clarity[T.VS1]:Color[T.F]                            0.0510      0.009      5.766      0.000       0.034       0.068\n",
      "Clarity[T.VS2]:Color[T.F]                            0.0829      0.008     10.478      0.000       0.067       0.098\n",
      "Clarity[T.VVS1]:Color[T.F]                          -0.0666      0.015     -4.520      0.000      -0.095      -0.038\n",
      "Clarity[T.VVS2]:Color[T.F]                          -0.0254      0.010     -2.589      0.010      -0.045      -0.006\n",
      "Clarity[T.IF]:Color[T.G]                            -0.3528      0.012    -29.163      0.000      -0.376      -0.329\n",
      "Clarity[T.SI1]:Color[T.G]                            0.0995      0.007     14.320      0.000       0.086       0.113\n",
      "Clarity[T.VS1]:Color[T.G]                            0.0590      0.008      7.101      0.000       0.043       0.075\n",
      "Clarity[T.VS2]:Color[T.G]                            0.1204      0.008     15.874      0.000       0.106       0.135\n",
      "Clarity[T.VVS1]:Color[T.G]                          -0.1472      0.014    -10.464      0.000      -0.175      -0.120\n",
      "Clarity[T.VVS2]:Color[T.G]                          -0.0589      0.009     -6.405      0.000      -0.077      -0.041\n",
      "Clarity[T.IF]:Color[T.H]                            -0.4107      0.015    -27.037      0.000      -0.440      -0.381\n",
      "Clarity[T.SI1]:Color[T.H]                            0.1775      0.007     25.000      0.000       0.164       0.191\n",
      "Clarity[T.VS1]:Color[T.H]                            0.0430      0.009      4.897      0.000       0.026       0.060\n",
      "Clarity[T.VS2]:Color[T.H]                            0.1168      0.008     14.502      0.000       0.101       0.133\n",
      "Clarity[T.VVS1]:Color[T.H]                          -0.2134      0.016    -13.675      0.000      -0.244      -0.183\n",
      "Clarity[T.VVS2]:Color[T.H]                          -0.1243      0.010    -12.132      0.000      -0.144      -0.104\n",
      "Clarity[T.IF]:Color[T.I]                            -0.4564      0.016    -28.279      0.000      -0.488      -0.425\n",
      "Clarity[T.SI1]:Color[T.I]                            0.1976      0.008     26.050      0.000       0.183       0.212\n",
      "Clarity[T.VS1]:Color[T.I]                            0.0164      0.009      1.813      0.070      -0.001       0.034\n",
      "Clarity[T.VS2]:Color[T.I]                            0.0969      0.008     11.570      0.000       0.080       0.113\n",
      "Clarity[T.VVS1]:Color[T.I]                          -0.2161      0.017    -12.637      0.000      -0.250      -0.183\n",
      "Clarity[T.VVS2]:Color[T.I]                          -0.1285      0.011    -11.428      0.000      -0.151      -0.106\n",
      "Clarity[T.IF]:Cut[T.Good]                           -0.0448      0.048     -0.928      0.353      -0.139       0.050\n",
      "Clarity[T.SI1]:Cut[T.Good]                           0.0037      0.016      0.233      0.815      -0.028       0.035\n",
      "Clarity[T.VS1]:Cut[T.Good]                           0.0386      0.021      1.873      0.061      -0.002       0.079\n",
      "Clarity[T.VS2]:Cut[T.Good]                           0.0135      0.018      0.739      0.460      -0.022       0.049\n",
      "Clarity[T.VVS1]:Cut[T.Good]                          0.0101      0.063      0.161      0.872      -0.114       0.134\n",
      "Clarity[T.VVS2]:Cut[T.Good]                         -0.0054      0.027     -0.197      0.844      -0.059       0.048\n",
      "Clarity[T.IF]:Cut[T.Ideal]                          -0.4829      0.058     -8.360      0.000      -0.596      -0.370\n",
      "Clarity[T.SI1]:Cut[T.Ideal]                         -0.4470      0.038    -11.631      0.000      -0.522      -0.372\n",
      "Clarity[T.VS1]:Cut[T.Ideal]                         -0.3860      0.040     -9.568      0.000      -0.465      -0.307\n",
      "Clarity[T.VS2]:Cut[T.Ideal]                         -0.4157      0.039    -10.570      0.000      -0.493      -0.339\n",
      "Clarity[T.VVS1]:Cut[T.Ideal]                        -0.3621      0.070     -5.205      0.000      -0.498      -0.226\n",
      "Clarity[T.VVS2]:Cut[T.Ideal]                        -0.4075      0.044     -9.270      0.000      -0.494      -0.321\n",
      "Clarity[T.IF]:Cut[T.Signature-Ideal]                -0.5777      0.065     -8.865      0.000      -0.705      -0.450\n",
      "Clarity[T.SI1]:Cut[T.Signature-Ideal]               -0.5023      0.042    -12.090      0.000      -0.584      -0.421\n",
      "Clarity[T.VS1]:Cut[T.Signature-Ideal]               -0.4700      0.043    -10.986      0.000      -0.554      -0.386\n",
      "Clarity[T.VS2]:Cut[T.Signature-Ideal]               -0.4963      0.042    -11.805      0.000      -0.579      -0.414\n",
      "Clarity[T.VVS1]:Cut[T.Signature-Ideal]              -0.4388      0.073     -6.034      0.000      -0.581      -0.296\n",
      "Clarity[T.VVS2]:Cut[T.Signature-Ideal]              -0.4699      0.047    -10.041      0.000      -0.562      -0.378\n",
      "Clarity[T.IF]:Cut[T.Very Good]                      -0.0681      0.046     -1.496      0.135      -0.157       0.021\n",
      "Clarity[T.SI1]:Cut[T.Very Good]                     -0.0128      0.015     -0.843      0.399      -0.042       0.017\n",
      "Clarity[T.VS1]:Cut[T.Very Good]                      0.0386      0.020      1.973      0.049       0.000       0.077\n",
      "Clarity[T.VS2]:Cut[T.Very Good]                      0.0077      0.017      0.445      0.656      -0.026       0.042\n",
      "Clarity[T.VVS1]:Cut[T.Very Good]                     0.0781      0.060      1.304      0.192      -0.039       0.196\n",
      "Clarity[T.VVS2]:Cut[T.Very Good]                     0.0103      0.026      0.396      0.692      -0.041       0.061\n",
      "Clarity[T.IF]:Symmetry[T.G]                         -0.0277      0.016     -1.703      0.089      -0.060       0.004\n",
      "Clarity[T.SI1]:Symmetry[T.G]                        -0.0054      0.007     -0.745      0.456      -0.019       0.009\n",
      "Clarity[T.VS1]:Symmetry[T.G]                        -0.0101      0.009     -1.156      0.248      -0.027       0.007\n",
      "Clarity[T.VS2]:Symmetry[T.G]                        -0.0048      0.008     -0.605      0.545      -0.020       0.011\n",
      "Clarity[T.VVS1]:Symmetry[T.G]                       -0.0018      0.018     -0.100      0.920      -0.037       0.033\n",
      "Clarity[T.VVS2]:Symmetry[T.G]                       -0.0027      0.012     -0.232      0.817      -0.025       0.020\n",
      "Clarity[T.IF]:Symmetry[T.ID]                        -0.0435      0.074     -0.591      0.554      -0.188       0.101\n",
      "Clarity[T.SI1]:Symmetry[T.ID]                        0.0069      0.035      0.199      0.843      -0.061       0.075\n",
      "Clarity[T.VS1]:Symmetry[T.ID]                       -0.0135      0.033     -0.411      0.681      -0.078       0.051\n",
      "Clarity[T.VS2]:Symmetry[T.ID]                        0.0118      0.039      0.299      0.765      -0.065       0.089\n",
      "Clarity[T.VVS1]:Symmetry[T.ID]                      -0.0033      0.048     -0.069      0.945      -0.097       0.091\n",
      "Clarity[T.VVS2]:Symmetry[T.ID]                      -0.0099      0.036     -0.277      0.782      -0.080       0.060\n",
      "Clarity[T.IF]:Symmetry[T.VG]                        -0.0297      0.011     -2.687      0.007      -0.051      -0.008\n",
      "Clarity[T.SI1]:Symmetry[T.VG]                       -0.0049      0.005     -0.889      0.374      -0.016       0.006\n",
      "Clarity[T.VS1]:Symmetry[T.VG]                       -0.0094      0.006     -1.485      0.138      -0.022       0.003\n",
      "Clarity[T.VS2]:Symmetry[T.VG]                       -0.0137      0.006     -2.368      0.018      -0.025      -0.002\n",
      "Clarity[T.VVS1]:Symmetry[T.VG]                       0.0016      0.010      0.155      0.877      -0.018       0.021\n",
      "Clarity[T.VVS2]:Symmetry[T.VG]                       0.0159      0.007      2.183      0.029       0.002       0.030\n",
      "Symmetry[T.G]:Color[T.E]                         -2.889e-05      0.013     -0.002      0.998      -0.025       0.025\n",
      "Symmetry[T.ID]:Color[T.E]                            0.1166      0.112      1.043      0.297      -0.102       0.336\n",
      "Symmetry[T.VG]:Color[T.E]                            0.0016      0.009      0.176      0.861      -0.017       0.020\n",
      "Symmetry[T.G]:Color[T.F]                             0.0086      0.012      0.695      0.487      -0.016       0.033\n",
      "Symmetry[T.ID]:Color[T.F]                            0.0939      0.063      1.501      0.133      -0.029       0.217\n",
      "Symmetry[T.VG]:Color[T.F]                            0.0167      0.009      1.894      0.058      -0.001       0.034\n",
      "Symmetry[T.G]:Color[T.G]                             0.0116      0.012      0.971      0.331      -0.012       0.035\n",
      "Symmetry[T.ID]:Color[T.G]                            0.0656      0.066      0.993      0.321      -0.064       0.195\n",
      "Symmetry[T.VG]:Color[T.G]                            0.0105      0.008      1.256      0.209      -0.006       0.027\n",
      "Symmetry[T.G]:Color[T.H]                             0.0205      0.012      1.649      0.099      -0.004       0.045\n",
      "Symmetry[T.ID]:Color[T.H]                            0.0744      0.063      1.186      0.236      -0.049       0.197\n",
      "Symmetry[T.VG]:Color[T.H]                            0.0193      0.009      2.200      0.028       0.002       0.036\n",
      "Symmetry[T.G]:Color[T.I]                             0.0126      0.013      0.963      0.335      -0.013       0.038\n",
      "Symmetry[T.ID]:Color[T.I]                            0.0443      0.068      0.650      0.516      -0.089       0.178\n",
      "Symmetry[T.VG]:Color[T.I]                            0.0121      0.009      1.295      0.195      -0.006       0.030\n",
      "Symmetry[T.G]:Cut[T.Good]                           -0.0117      0.024     -0.481      0.631      -0.059       0.036\n",
      "Symmetry[T.ID]:Cut[T.Good]                          -0.0405      0.074     -0.548      0.583      -0.185       0.104\n",
      "Symmetry[T.VG]:Cut[T.Good]                          -0.0167      0.024     -0.692      0.489      -0.064       0.031\n",
      "Symmetry[T.G]:Cut[T.Ideal]                           0.0107      0.024      0.440      0.660      -0.037       0.059\n",
      "Symmetry[T.ID]:Cut[T.Ideal]                         -0.0395      0.057     -0.694      0.488      -0.151       0.072\n",
      "Symmetry[T.VG]:Cut[T.Ideal]                         -0.0033      0.023     -0.148      0.882      -0.048       0.041\n",
      "Symmetry[T.G]:Cut[T.Signature-Ideal]             -1.572e-16   3.68e-17     -4.275      0.000   -2.29e-16   -8.51e-17\n",
      "Symmetry[T.ID]:Cut[T.Signature-Ideal]                0.0126      0.012      1.054      0.292      -0.011       0.036\n",
      "Symmetry[T.VG]:Cut[T.Signature-Ideal]             1.819e-16   3.37e-17      5.397      0.000    1.16e-16    2.48e-16\n",
      "Symmetry[T.G]:Cut[T.Very Good]                       0.0008      0.023      0.036      0.971      -0.044       0.045\n",
      "Symmetry[T.ID]:Cut[T.Very Good]                      0.0158      0.030      0.528      0.597      -0.043       0.075\n",
      "Symmetry[T.VG]:Cut[T.Very Good]                      0.0066      0.023      0.291      0.771      -0.038       0.051\n",
      "Color[T.E]:Cut[T.Good]                              -0.0030      0.025     -0.122      0.903      -0.052       0.046\n",
      "Color[T.F]:Cut[T.Good]                              -0.0241      0.026     -0.924      0.356      -0.075       0.027\n",
      "Color[T.G]:Cut[T.Good]                               0.0281      0.027      1.032      0.302      -0.025       0.081\n",
      "Color[T.H]:Cut[T.Good]                               0.0341      0.026      1.297      0.195      -0.017       0.086\n",
      "Color[T.I]:Cut[T.Good]                              -0.0005      0.029     -0.018      0.986      -0.057       0.056\n",
      "Color[T.E]:Cut[T.Ideal]                             -0.0051      0.024     -0.210      0.834      -0.052       0.042\n",
      "Color[T.F]:Cut[T.Ideal]                             -0.0191      0.026     -0.745      0.456      -0.069       0.031\n",
      "Color[T.G]:Cut[T.Ideal]                              0.0166      0.027      0.621      0.534      -0.036       0.069\n",
      "Color[T.H]:Cut[T.Ideal]                              0.0308      0.026      1.199      0.231      -0.020       0.081\n",
      "Color[T.I]:Cut[T.Ideal]                             -0.0246      0.028     -0.871      0.384      -0.080       0.031\n",
      "Color[T.E]:Cut[T.Signature-Ideal]                   -0.0190      0.032     -0.601      0.548      -0.081       0.043\n",
      "Color[T.F]:Cut[T.Signature-Ideal]                   -0.0392      0.032     -1.229      0.219      -0.102       0.023\n",
      "Color[T.G]:Cut[T.Signature-Ideal]                   -0.0096      0.032     -0.298      0.766      -0.073       0.053\n",
      "Color[T.H]:Cut[T.Signature-Ideal]                    0.0018      0.032      0.058      0.954      -0.061       0.064\n",
      "Color[T.I]:Cut[T.Signature-Ideal]                   -0.0422      0.034     -1.243      0.214      -0.109       0.024\n",
      "Color[T.E]:Cut[T.Very Good]                         -0.0020      0.024     -0.084      0.933      -0.048       0.044\n",
      "Color[T.F]:Cut[T.Very Good]                         -0.0161      0.025     -0.643      0.521      -0.065       0.033\n",
      "Color[T.G]:Cut[T.Very Good]                          0.0265      0.026      1.011      0.312      -0.025       0.078\n",
      "Color[T.H]:Cut[T.Very Good]                          0.0378      0.025      1.501      0.133      -0.012       0.087\n",
      "Color[T.I]:Cut[T.Very Good]                         -0.0041      0.028     -0.150      0.881      -0.058       0.050\n",
      "Q(\"Carat Weight\")                                    2.6270      0.060     43.638      0.000       2.509       2.745\n",
      "np.log(Q(\"Carat Weight\"))                            0.1790      0.057      3.125      0.002       0.067       0.291\n",
      "Symmetry[T.G]:np.log(Q(\"Carat Weight\"))              0.0268      0.010      2.585      0.010       0.006       0.047\n",
      "Symmetry[T.ID]:np.log(Q(\"Carat Weight\"))            -0.0227      0.047     -0.488      0.626      -0.114       0.068\n",
      "Symmetry[T.VG]:np.log(Q(\"Carat Weight\"))             0.0160      0.007      2.230      0.026       0.002       0.030\n",
      "Polish[T.G]:np.log(Q(\"Carat Weight\"))               -0.0014      0.011     -0.121      0.904      -0.024       0.021\n",
      "Polish[T.ID]:np.log(Q(\"Carat Weight\"))               0.0084      0.046      0.184      0.854      -0.081       0.098\n",
      "Polish[T.VG]:np.log(Q(\"Carat Weight\"))              -0.0007      0.007     -0.102      0.919      -0.014       0.013\n",
      "Color[T.E]:np.log(Q(\"Carat Weight\"))                -0.0242      0.011     -2.116      0.034      -0.047      -0.002\n",
      "Color[T.F]:np.log(Q(\"Carat Weight\"))                -0.0086      0.011     -0.797      0.425      -0.030       0.012\n",
      "Color[T.G]:np.log(Q(\"Carat Weight\"))                -0.0474      0.010     -4.689      0.000      -0.067      -0.028\n",
      "Color[T.H]:np.log(Q(\"Carat Weight\"))                -0.1433      0.011    -13.587      0.000      -0.164      -0.123\n",
      "Color[T.I]:np.log(Q(\"Carat Weight\"))                -0.1756      0.011    -16.154      0.000      -0.197      -0.154\n",
      "Cut[T.Good]:np.log(Q(\"Carat Weight\"))                0.0371      0.031      1.208      0.227      -0.023       0.097\n",
      "Cut[T.Ideal]:np.log(Q(\"Carat Weight\"))               0.0012      0.031      0.041      0.968      -0.059       0.061\n",
      "Cut[T.Signature-Ideal]:np.log(Q(\"Carat Weight\"))    -0.0102      0.036     -0.281      0.778      -0.081       0.061\n",
      "Cut[T.Very Good]:np.log(Q(\"Carat Weight\"))           0.0257      0.030      0.851      0.395      -0.033       0.085\n",
      "Report[T.GIA]:np.log(Q(\"Carat Weight\"))             -0.0258      0.016     -1.598      0.110      -0.057       0.006\n",
      "np.maximum(Q(\"Carat Weight\") - 1, 0)                -1.3466      0.031    -43.825      0.000      -1.407      -1.286\n",
      "np.maximum(Q(\"Carat Weight\") - 2, 0)                -0.6823      0.019    -35.662      0.000      -0.720      -0.645\n",
      "==============================================================================\n",
      "Omnibus:                      593.957   Durbin-Watson:                   1.969\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             4086.434\n",
      "Skew:                          -0.193   Prob(JB):                         0.00\n",
      "Kurtosis:                       7.025   Cond. No.                     1.39e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.81e-28. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "Mean Absolute Error for validation data: 600.3999407474377\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Fit the model using train data\n",
    "formula = 'np.log(Price_numeric) ~ Q(\"Carat Weight\") + Cut + Clarity + Report + Color + Symmetry + Polish + Polish * Clarity + Polish * Cut + Polish * Symmetry + Polish * Color + Clarity * Color + Clarity * Cut + Clarity * Symmetry + Symmetry * Color + Symmetry * Cut + Color * Cut + np.log(Q(\"Carat Weight\")) + Symmetry*np.log(Q(\"Carat Weight\")) + Polish*np.log(Q(\"Carat Weight\")) + Color*np.log(Q(\"Carat Weight\")) + Cut*np.log(Q(\"Carat Weight\")) + np.maximum(Q(\"Carat Weight\") - 1, 0) + np.maximum(Q(\"Carat Weight\") - 2, 0) + Report*np.log(Q(\"Carat Weight\"))'\n",
    "lm_9 = smf.ols(formula=formula, data=df_train).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(lm_9.summary())\n",
    "\n",
    "# Step 2: Calculate predictions for train data and add them as a new column\n",
    "df_train['Price_Predictions'] = np.exp(lm_9.predict(df_train)) # Using np.exp() to convert log predictions back to original scale\n",
    "\n",
    "# If test_df has a 'Price_numeric' column, we can make predictions for it as well:\n",
    "if 'Price_numeric' in test_df.columns:\n",
    "    test_df['Price_Predictions'] = np.exp(lm_9.predict(test_df))\n",
    "\n",
    "# Step 3: Compute mean absolute error for validation data\n",
    "# Assuming you have a separate validation set named df_validation with actual price information\n",
    "if 'Price_numeric' in df_validation.columns:\n",
    "    lm_9_predictions = lm_9.predict(df_validation)\n",
    "    error = mean_absolute_error(df_validation[\"Price_numeric\"], np.exp(lm_9_predictions))\n",
    "    print(f\"Mean Absolute Error for validation data: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "921325ed-efc0-419a-90fd-a3e5ae3de193",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              OLS Regression Results                             \n",
      "=================================================================================\n",
      "Dep. Variable:     np.log(Price_numeric)   R-squared:                       0.992\n",
      "Model:                               OLS   Adj. R-squared:                  0.992\n",
      "Method:                    Least Squares   F-statistic:                     3710.\n",
      "Date:                   Thu, 26 Oct 2023   Prob (F-statistic):               0.00\n",
      "Time:                           21:36:37   Log-Likelihood:                 7942.9\n",
      "No. Observations:                   6000   AIC:                        -1.551e+04\n",
      "Df Residuals:                       5810   BIC:                        -1.423e+04\n",
      "Df Model:                            189                                         \n",
      "Covariance Type:               nonrobust                                         \n",
      "====================================================================================================================\n",
      "                                                       coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                            6.4443      0.067     96.481      0.000       6.313       6.575\n",
      "Cut[T.Good]                                          0.0158      0.029      0.550      0.582      -0.041       0.072\n",
      "Cut[T.Ideal]                                         0.5394      0.045     12.075      0.000       0.452       0.627\n",
      "Cut[T.Signature-Ideal]                               0.7525      0.045     16.575      0.000       0.663       0.841\n",
      "Cut[T.Very Good]                                     0.0539      0.027      1.966      0.049       0.000       0.108\n",
      "Clarity[T.IF]                                        0.5182      0.053      9.784      0.000       0.414       0.622\n",
      "Clarity[T.SI1]                                      -0.4646      0.028    -16.468      0.000      -0.520      -0.409\n",
      "Clarity[T.VS1]                                      -0.2042      0.031     -6.588      0.000      -0.265      -0.143\n",
      "Clarity[T.VS2]                                      -0.3057      0.030    -10.343      0.000      -0.364      -0.248\n",
      "Clarity[T.VVS1]                                      0.1225      0.066      1.861      0.063      -0.007       0.252\n",
      "Clarity[T.VVS2]                                      0.0301      0.036      0.840      0.401      -0.040       0.100\n",
      "Report[T.GIA]                                        0.0370      0.007      5.040      0.000       0.023       0.051\n",
      "Color[T.E]                                          -0.0830      0.021     -3.888      0.000      -0.125      -0.041\n",
      "Color[T.F]                                          -0.1314      0.022     -5.896      0.000      -0.175      -0.088\n",
      "Color[T.G]                                          -0.2800      0.023    -12.063      0.000      -0.325      -0.234\n",
      "Color[T.H]                                          -0.4111      0.022    -18.347      0.000      -0.455      -0.367\n",
      "Color[T.I]                                          -0.4901      0.025    -19.886      0.000      -0.538      -0.442\n",
      "Symmetry[T.G]                                       -0.0524      0.021     -2.458      0.014      -0.094      -0.011\n",
      "Symmetry[T.ID]                                      -0.0515      0.048     -1.070      0.285      -0.146       0.043\n",
      "Symmetry[T.VG]                                      -0.0402      0.020     -1.990      0.047      -0.080      -0.001\n",
      "Polish[T.G]                                         -0.0515      0.024     -2.143      0.032      -0.099      -0.004\n",
      "Polish[T.ID]                                         0.0699      0.044      1.594      0.111      -0.016       0.156\n",
      "Polish[T.VG]                                        -0.0259      0.016     -1.594      0.111      -0.058       0.006\n",
      "Polish[T.G]:Clarity[T.IF]                           -0.0043      0.020     -0.221      0.825      -0.043       0.034\n",
      "Polish[T.ID]:Clarity[T.IF]                           0.0297      0.071      0.419      0.675      -0.109       0.169\n",
      "Polish[T.VG]:Clarity[T.IF]                           0.0080      0.010      0.779      0.436      -0.012       0.028\n",
      "Polish[T.G]:Clarity[T.SI1]                          -0.0126      0.008     -1.589      0.112      -0.028       0.003\n",
      "Polish[T.ID]:Clarity[T.SI1]                          0.0174      0.035      0.504      0.614      -0.050       0.085\n",
      "Polish[T.VG]:Clarity[T.SI1]                         -0.0113      0.005     -2.328      0.020      -0.021      -0.002\n",
      "Polish[T.G]:Clarity[T.VS1]                          -0.0114      0.010     -1.144      0.253      -0.031       0.008\n",
      "Polish[T.ID]:Clarity[T.VS1]                          0.0150      0.032      0.475      0.635      -0.047       0.077\n",
      "Polish[T.VG]:Clarity[T.VS1]                         -0.0040      0.006     -0.693      0.488      -0.015       0.007\n",
      "Polish[T.G]:Clarity[T.VS2]                          -0.0133      0.009     -1.500      0.134      -0.031       0.004\n",
      "Polish[T.ID]:Clarity[T.VS2]                         -0.0061      0.041     -0.150      0.881      -0.086       0.074\n",
      "Polish[T.VG]:Clarity[T.VS2]                         -0.0096      0.005     -1.839      0.066      -0.020       0.001\n",
      "Polish[T.G]:Clarity[T.VVS1]                          0.0057      0.022      0.252      0.801      -0.038       0.050\n",
      "Polish[T.ID]:Clarity[T.VVS1]                         0.0012      0.049      0.024      0.981      -0.095       0.098\n",
      "Polish[T.VG]:Clarity[T.VVS1]                        -0.0129      0.010     -1.268      0.205      -0.033       0.007\n",
      "Polish[T.G]:Clarity[T.VVS2]                         -0.0156      0.013     -1.228      0.219      -0.040       0.009\n",
      "Polish[T.ID]:Clarity[T.VVS2]                         0.0127      0.037      0.341      0.733      -0.060       0.085\n",
      "Polish[T.VG]:Clarity[T.VVS2]                         0.0040      0.007      0.592      0.554      -0.009       0.017\n",
      "Polish[T.G]:Cut[T.Good]                              0.0370      0.023      1.640      0.101      -0.007       0.081\n",
      "Polish[T.ID]:Cut[T.Good]                          8.275e-16   3.42e-16      2.419      0.016    1.57e-16     1.5e-15\n",
      "Polish[T.VG]:Cut[T.Good]                             0.0186      0.018      1.014      0.311      -0.017       0.054\n",
      "Polish[T.G]:Cut[T.Ideal]                            -0.0014      0.024     -0.059      0.953      -0.049       0.046\n",
      "Polish[T.ID]:Cut[T.Ideal]                            0.0522      0.051      1.021      0.308      -0.048       0.152\n",
      "Polish[T.VG]:Cut[T.Ideal]                           -0.0022      0.018     -0.125      0.900      -0.037       0.033\n",
      "Polish[T.G]:Cut[T.Signature-Ideal]               -7.526e-16   1.72e-16     -4.371      0.000   -1.09e-15   -4.15e-16\n",
      "Polish[T.ID]:Cut[T.Signature-Ideal]                  0.0126      0.012      1.054      0.292      -0.011       0.036\n",
      "Polish[T.VG]:Cut[T.Signature-Ideal]               1.521e-15   2.33e-16      6.532      0.000    1.06e-15    1.98e-15\n",
      "Polish[T.G]:Cut[T.Very Good]                         0.0109      0.022      0.500      0.617      -0.032       0.054\n",
      "Polish[T.ID]:Cut[T.Very Good]                        0.0051      0.026      0.193      0.847      -0.047       0.057\n",
      "Polish[T.VG]:Cut[T.Very Good]                        0.0109      0.018      0.619      0.536      -0.024       0.045\n",
      "Polish[T.G]:Symmetry[T.G]                            0.0354      0.015      2.353      0.019       0.006       0.065\n",
      "Polish[T.ID]:Symmetry[T.G]                       -5.939e-16   2.75e-16     -2.162      0.031   -1.13e-15   -5.55e-17\n",
      "Polish[T.VG]:Symmetry[T.G]                           0.0212      0.009      2.386      0.017       0.004       0.039\n",
      "Polish[T.G]:Symmetry[T.ID]                       -1.807e-15   3.33e-16     -5.431      0.000   -2.46e-15   -1.15e-15\n",
      "Polish[T.ID]:Symmetry[T.ID]                         -0.0123      0.041     -0.303      0.762      -0.092       0.068\n",
      "Polish[T.VG]:Symmetry[T.ID]                      -9.812e-16   2.88e-16     -3.403      0.001   -1.55e-15   -4.16e-16\n",
      "Polish[T.G]:Symmetry[T.VG]                           0.0250      0.013      1.855      0.064      -0.001       0.051\n",
      "Polish[T.ID]:Symmetry[T.VG]                      -1.458e-15   3.13e-16     -4.661      0.000   -2.07e-15   -8.45e-16\n",
      "Polish[T.VG]:Symmetry[T.VG]                          0.0229      0.005      4.628      0.000       0.013       0.033\n",
      "Polish[T.G]:Color[T.E]                              -0.0053      0.014     -0.375      0.707      -0.033       0.022\n",
      "Polish[T.ID]:Color[T.E]                             -0.1343      0.113     -1.193      0.233      -0.355       0.086\n",
      "Polish[T.VG]:Color[T.E]                             -0.0049      0.009     -0.552      0.581      -0.022       0.012\n",
      "Polish[T.G]:Color[T.F]                              -0.0200      0.013     -1.485      0.138      -0.046       0.006\n",
      "Polish[T.ID]:Color[T.F]                             -0.1105      0.063     -1.754      0.080      -0.234       0.013\n",
      "Polish[T.VG]:Color[T.F]                             -0.0180      0.008     -2.159      0.031      -0.034      -0.002\n",
      "Polish[T.G]:Color[T.G]                               0.0016      0.013      0.121      0.904      -0.024       0.027\n",
      "Polish[T.ID]:Color[T.G]                             -0.0826      0.066     -1.247      0.213      -0.212       0.047\n",
      "Polish[T.VG]:Color[T.G]                             -0.0035      0.008     -0.445      0.656      -0.019       0.012\n",
      "Polish[T.G]:Color[T.H]                               0.0022      0.014      0.158      0.874      -0.025       0.029\n",
      "Polish[T.ID]:Color[T.H]                             -0.0825      0.063     -1.304      0.192      -0.206       0.042\n",
      "Polish[T.VG]:Color[T.H]                             -0.0100      0.008     -1.198      0.231      -0.026       0.006\n",
      "Polish[T.G]:Color[T.I]                               0.0042      0.014      0.302      0.762      -0.023       0.032\n",
      "Polish[T.ID]:Color[T.I]                             -0.0512      0.068     -0.749      0.454      -0.185       0.083\n",
      "Polish[T.VG]:Color[T.I]                             -0.0037      0.009     -0.419      0.675      -0.021       0.014\n",
      "Clarity[T.IF]:Color[T.E]                            -0.1835      0.017    -10.602      0.000      -0.217      -0.150\n",
      "Clarity[T.SI1]:Color[T.E]                            0.0368      0.007      5.068      0.000       0.023       0.051\n",
      "Clarity[T.VS1]:Color[T.E]                            0.0315      0.010      3.279      0.001       0.013       0.050\n",
      "Clarity[T.VS2]:Color[T.E]                            0.0598      0.008      7.073      0.000       0.043       0.076\n",
      "Clarity[T.VVS1]:Color[T.E]                          -0.0079      0.017     -0.474      0.636      -0.041       0.025\n",
      "Clarity[T.VVS2]:Color[T.E]                          -0.0198      0.010     -1.886      0.059      -0.040       0.001\n",
      "Clarity[T.IF]:Color[T.F]                            -0.2268      0.014    -16.601      0.000      -0.254      -0.200\n",
      "Clarity[T.SI1]:Color[T.F]                            0.0535      0.007      7.674      0.000       0.040       0.067\n",
      "Clarity[T.VS1]:Color[T.F]                            0.0510      0.009      5.766      0.000       0.034       0.068\n",
      "Clarity[T.VS2]:Color[T.F]                            0.0829      0.008     10.478      0.000       0.067       0.098\n",
      "Clarity[T.VVS1]:Color[T.F]                          -0.0666      0.015     -4.520      0.000      -0.095      -0.038\n",
      "Clarity[T.VVS2]:Color[T.F]                          -0.0254      0.010     -2.589      0.010      -0.045      -0.006\n",
      "Clarity[T.IF]:Color[T.G]                            -0.3528      0.012    -29.163      0.000      -0.376      -0.329\n",
      "Clarity[T.SI1]:Color[T.G]                            0.0995      0.007     14.320      0.000       0.086       0.113\n",
      "Clarity[T.VS1]:Color[T.G]                            0.0590      0.008      7.101      0.000       0.043       0.075\n",
      "Clarity[T.VS2]:Color[T.G]                            0.1204      0.008     15.874      0.000       0.106       0.135\n",
      "Clarity[T.VVS1]:Color[T.G]                          -0.1472      0.014    -10.464      0.000      -0.175      -0.120\n",
      "Clarity[T.VVS2]:Color[T.G]                          -0.0589      0.009     -6.405      0.000      -0.077      -0.041\n",
      "Clarity[T.IF]:Color[T.H]                            -0.4107      0.015    -27.037      0.000      -0.440      -0.381\n",
      "Clarity[T.SI1]:Color[T.H]                            0.1775      0.007     25.000      0.000       0.164       0.191\n",
      "Clarity[T.VS1]:Color[T.H]                            0.0430      0.009      4.897      0.000       0.026       0.060\n",
      "Clarity[T.VS2]:Color[T.H]                            0.1168      0.008     14.502      0.000       0.101       0.133\n",
      "Clarity[T.VVS1]:Color[T.H]                          -0.2134      0.016    -13.675      0.000      -0.244      -0.183\n",
      "Clarity[T.VVS2]:Color[T.H]                          -0.1243      0.010    -12.132      0.000      -0.144      -0.104\n",
      "Clarity[T.IF]:Color[T.I]                            -0.4564      0.016    -28.279      0.000      -0.488      -0.425\n",
      "Clarity[T.SI1]:Color[T.I]                            0.1976      0.008     26.050      0.000       0.183       0.212\n",
      "Clarity[T.VS1]:Color[T.I]                            0.0164      0.009      1.813      0.070      -0.001       0.034\n",
      "Clarity[T.VS2]:Color[T.I]                            0.0969      0.008     11.570      0.000       0.080       0.113\n",
      "Clarity[T.VVS1]:Color[T.I]                          -0.2161      0.017    -12.637      0.000      -0.250      -0.183\n",
      "Clarity[T.VVS2]:Color[T.I]                          -0.1285      0.011    -11.428      0.000      -0.151      -0.106\n",
      "Clarity[T.IF]:Cut[T.Good]                           -0.0448      0.048     -0.928      0.353      -0.139       0.050\n",
      "Clarity[T.SI1]:Cut[T.Good]                           0.0037      0.016      0.233      0.815      -0.028       0.035\n",
      "Clarity[T.VS1]:Cut[T.Good]                           0.0386      0.021      1.873      0.061      -0.002       0.079\n",
      "Clarity[T.VS2]:Cut[T.Good]                           0.0135      0.018      0.739      0.460      -0.022       0.049\n",
      "Clarity[T.VVS1]:Cut[T.Good]                          0.0101      0.063      0.161      0.872      -0.114       0.134\n",
      "Clarity[T.VVS2]:Cut[T.Good]                         -0.0054      0.027     -0.197      0.844      -0.059       0.048\n",
      "Clarity[T.IF]:Cut[T.Ideal]                          -0.4829      0.058     -8.360      0.000      -0.596      -0.370\n",
      "Clarity[T.SI1]:Cut[T.Ideal]                         -0.4470      0.038    -11.631      0.000      -0.522      -0.372\n",
      "Clarity[T.VS1]:Cut[T.Ideal]                         -0.3860      0.040     -9.568      0.000      -0.465      -0.307\n",
      "Clarity[T.VS2]:Cut[T.Ideal]                         -0.4157      0.039    -10.570      0.000      -0.493      -0.339\n",
      "Clarity[T.VVS1]:Cut[T.Ideal]                        -0.3621      0.070     -5.205      0.000      -0.498      -0.226\n",
      "Clarity[T.VVS2]:Cut[T.Ideal]                        -0.4075      0.044     -9.270      0.000      -0.494      -0.321\n",
      "Clarity[T.IF]:Cut[T.Signature-Ideal]                -0.5777      0.065     -8.865      0.000      -0.705      -0.450\n",
      "Clarity[T.SI1]:Cut[T.Signature-Ideal]               -0.5023      0.042    -12.090      0.000      -0.584      -0.421\n",
      "Clarity[T.VS1]:Cut[T.Signature-Ideal]               -0.4700      0.043    -10.986      0.000      -0.554      -0.386\n",
      "Clarity[T.VS2]:Cut[T.Signature-Ideal]               -0.4963      0.042    -11.805      0.000      -0.579      -0.414\n",
      "Clarity[T.VVS1]:Cut[T.Signature-Ideal]              -0.4388      0.073     -6.034      0.000      -0.581      -0.296\n",
      "Clarity[T.VVS2]:Cut[T.Signature-Ideal]              -0.4699      0.047    -10.041      0.000      -0.562      -0.378\n",
      "Clarity[T.IF]:Cut[T.Very Good]                      -0.0681      0.046     -1.496      0.135      -0.157       0.021\n",
      "Clarity[T.SI1]:Cut[T.Very Good]                     -0.0128      0.015     -0.843      0.399      -0.042       0.017\n",
      "Clarity[T.VS1]:Cut[T.Very Good]                      0.0386      0.020      1.973      0.049       0.000       0.077\n",
      "Clarity[T.VS2]:Cut[T.Very Good]                      0.0077      0.017      0.445      0.656      -0.026       0.042\n",
      "Clarity[T.VVS1]:Cut[T.Very Good]                     0.0781      0.060      1.304      0.192      -0.039       0.196\n",
      "Clarity[T.VVS2]:Cut[T.Very Good]                     0.0103      0.026      0.396      0.692      -0.041       0.061\n",
      "Clarity[T.IF]:Symmetry[T.G]                         -0.0277      0.016     -1.703      0.089      -0.060       0.004\n",
      "Clarity[T.SI1]:Symmetry[T.G]                        -0.0054      0.007     -0.745      0.456      -0.019       0.009\n",
      "Clarity[T.VS1]:Symmetry[T.G]                        -0.0101      0.009     -1.156      0.248      -0.027       0.007\n",
      "Clarity[T.VS2]:Symmetry[T.G]                        -0.0048      0.008     -0.605      0.545      -0.020       0.011\n",
      "Clarity[T.VVS1]:Symmetry[T.G]                       -0.0018      0.018     -0.100      0.920      -0.037       0.033\n",
      "Clarity[T.VVS2]:Symmetry[T.G]                       -0.0027      0.012     -0.232      0.817      -0.025       0.020\n",
      "Clarity[T.IF]:Symmetry[T.ID]                        -0.0435      0.074     -0.591      0.554      -0.188       0.101\n",
      "Clarity[T.SI1]:Symmetry[T.ID]                        0.0069      0.035      0.199      0.843      -0.061       0.075\n",
      "Clarity[T.VS1]:Symmetry[T.ID]                       -0.0135      0.033     -0.411      0.681      -0.078       0.051\n",
      "Clarity[T.VS2]:Symmetry[T.ID]                        0.0118      0.039      0.299      0.765      -0.065       0.089\n",
      "Clarity[T.VVS1]:Symmetry[T.ID]                      -0.0033      0.048     -0.069      0.945      -0.097       0.091\n",
      "Clarity[T.VVS2]:Symmetry[T.ID]                      -0.0099      0.036     -0.277      0.782      -0.080       0.060\n",
      "Clarity[T.IF]:Symmetry[T.VG]                        -0.0297      0.011     -2.687      0.007      -0.051      -0.008\n",
      "Clarity[T.SI1]:Symmetry[T.VG]                       -0.0049      0.005     -0.889      0.374      -0.016       0.006\n",
      "Clarity[T.VS1]:Symmetry[T.VG]                       -0.0094      0.006     -1.485      0.138      -0.022       0.003\n",
      "Clarity[T.VS2]:Symmetry[T.VG]                       -0.0137      0.006     -2.368      0.018      -0.025      -0.002\n",
      "Clarity[T.VVS1]:Symmetry[T.VG]                       0.0016      0.010      0.155      0.877      -0.018       0.021\n",
      "Clarity[T.VVS2]:Symmetry[T.VG]                       0.0159      0.007      2.183      0.029       0.002       0.030\n",
      "Symmetry[T.G]:Color[T.E]                         -2.889e-05      0.013     -0.002      0.998      -0.025       0.025\n",
      "Symmetry[T.ID]:Color[T.E]                            0.1166      0.112      1.043      0.297      -0.102       0.336\n",
      "Symmetry[T.VG]:Color[T.E]                            0.0016      0.009      0.176      0.861      -0.017       0.020\n",
      "Symmetry[T.G]:Color[T.F]                             0.0086      0.012      0.695      0.487      -0.016       0.033\n",
      "Symmetry[T.ID]:Color[T.F]                            0.0939      0.063      1.501      0.133      -0.029       0.217\n",
      "Symmetry[T.VG]:Color[T.F]                            0.0167      0.009      1.894      0.058      -0.001       0.034\n",
      "Symmetry[T.G]:Color[T.G]                             0.0116      0.012      0.971      0.331      -0.012       0.035\n",
      "Symmetry[T.ID]:Color[T.G]                            0.0656      0.066      0.993      0.321      -0.064       0.195\n",
      "Symmetry[T.VG]:Color[T.G]                            0.0105      0.008      1.256      0.209      -0.006       0.027\n",
      "Symmetry[T.G]:Color[T.H]                             0.0205      0.012      1.649      0.099      -0.004       0.045\n",
      "Symmetry[T.ID]:Color[T.H]                            0.0744      0.063      1.186      0.236      -0.049       0.197\n",
      "Symmetry[T.VG]:Color[T.H]                            0.0193      0.009      2.200      0.028       0.002       0.036\n",
      "Symmetry[T.G]:Color[T.I]                             0.0126      0.013      0.963      0.335      -0.013       0.038\n",
      "Symmetry[T.ID]:Color[T.I]                            0.0443      0.068      0.650      0.516      -0.089       0.178\n",
      "Symmetry[T.VG]:Color[T.I]                            0.0121      0.009      1.295      0.195      -0.006       0.030\n",
      "Symmetry[T.G]:Cut[T.Good]                           -0.0117      0.024     -0.481      0.631      -0.059       0.036\n",
      "Symmetry[T.ID]:Cut[T.Good]                          -0.0405      0.074     -0.548      0.583      -0.185       0.104\n",
      "Symmetry[T.VG]:Cut[T.Good]                          -0.0167      0.024     -0.692      0.489      -0.064       0.031\n",
      "Symmetry[T.G]:Cut[T.Ideal]                           0.0107      0.024      0.440      0.660      -0.037       0.059\n",
      "Symmetry[T.ID]:Cut[T.Ideal]                         -0.0395      0.057     -0.694      0.488      -0.151       0.072\n",
      "Symmetry[T.VG]:Cut[T.Ideal]                         -0.0033      0.023     -0.148      0.882      -0.048       0.041\n",
      "Symmetry[T.G]:Cut[T.Signature-Ideal]             -1.572e-16   3.68e-17     -4.275      0.000   -2.29e-16   -8.51e-17\n",
      "Symmetry[T.ID]:Cut[T.Signature-Ideal]                0.0126      0.012      1.054      0.292      -0.011       0.036\n",
      "Symmetry[T.VG]:Cut[T.Signature-Ideal]             1.819e-16   3.37e-17      5.397      0.000    1.16e-16    2.48e-16\n",
      "Symmetry[T.G]:Cut[T.Very Good]                       0.0008      0.023      0.036      0.971      -0.044       0.045\n",
      "Symmetry[T.ID]:Cut[T.Very Good]                      0.0158      0.030      0.528      0.597      -0.043       0.075\n",
      "Symmetry[T.VG]:Cut[T.Very Good]                      0.0066      0.023      0.291      0.771      -0.038       0.051\n",
      "Color[T.E]:Cut[T.Good]                              -0.0030      0.025     -0.122      0.903      -0.052       0.046\n",
      "Color[T.F]:Cut[T.Good]                              -0.0241      0.026     -0.924      0.356      -0.075       0.027\n",
      "Color[T.G]:Cut[T.Good]                               0.0281      0.027      1.032      0.302      -0.025       0.081\n",
      "Color[T.H]:Cut[T.Good]                               0.0341      0.026      1.297      0.195      -0.017       0.086\n",
      "Color[T.I]:Cut[T.Good]                              -0.0005      0.029     -0.018      0.986      -0.057       0.056\n",
      "Color[T.E]:Cut[T.Ideal]                             -0.0051      0.024     -0.210      0.834      -0.052       0.042\n",
      "Color[T.F]:Cut[T.Ideal]                             -0.0191      0.026     -0.745      0.456      -0.069       0.031\n",
      "Color[T.G]:Cut[T.Ideal]                              0.0166      0.027      0.621      0.534      -0.036       0.069\n",
      "Color[T.H]:Cut[T.Ideal]                              0.0308      0.026      1.199      0.231      -0.020       0.081\n",
      "Color[T.I]:Cut[T.Ideal]                             -0.0246      0.028     -0.871      0.384      -0.080       0.031\n",
      "Color[T.E]:Cut[T.Signature-Ideal]                   -0.0190      0.032     -0.601      0.548      -0.081       0.043\n",
      "Color[T.F]:Cut[T.Signature-Ideal]                   -0.0392      0.032     -1.229      0.219      -0.102       0.023\n",
      "Color[T.G]:Cut[T.Signature-Ideal]                   -0.0096      0.032     -0.298      0.766      -0.073       0.053\n",
      "Color[T.H]:Cut[T.Signature-Ideal]                    0.0018      0.032      0.058      0.954      -0.061       0.064\n",
      "Color[T.I]:Cut[T.Signature-Ideal]                   -0.0422      0.034     -1.243      0.214      -0.109       0.024\n",
      "Color[T.E]:Cut[T.Very Good]                         -0.0020      0.024     -0.084      0.933      -0.048       0.044\n",
      "Color[T.F]:Cut[T.Very Good]                         -0.0161      0.025     -0.643      0.521      -0.065       0.033\n",
      "Color[T.G]:Cut[T.Very Good]                          0.0265      0.026      1.011      0.312      -0.025       0.078\n",
      "Color[T.H]:Cut[T.Very Good]                          0.0378      0.025      1.501      0.133      -0.012       0.087\n",
      "Color[T.I]:Cut[T.Very Good]                         -0.0041      0.028     -0.150      0.881      -0.058       0.050\n",
      "Q(\"Carat Weight\")                                    2.6270      0.060     43.638      0.000       2.509       2.745\n",
      "np.log(Q(\"Carat Weight\"))                            0.1790      0.057      3.125      0.002       0.067       0.291\n",
      "Symmetry[T.G]:np.log(Q(\"Carat Weight\"))              0.0268      0.010      2.585      0.010       0.006       0.047\n",
      "Symmetry[T.ID]:np.log(Q(\"Carat Weight\"))            -0.0227      0.047     -0.488      0.626      -0.114       0.068\n",
      "Symmetry[T.VG]:np.log(Q(\"Carat Weight\"))             0.0160      0.007      2.230      0.026       0.002       0.030\n",
      "Polish[T.G]:np.log(Q(\"Carat Weight\"))               -0.0014      0.011     -0.121      0.904      -0.024       0.021\n",
      "Polish[T.ID]:np.log(Q(\"Carat Weight\"))               0.0084      0.046      0.184      0.854      -0.081       0.098\n",
      "Polish[T.VG]:np.log(Q(\"Carat Weight\"))              -0.0007      0.007     -0.102      0.919      -0.014       0.013\n",
      "Color[T.E]:np.log(Q(\"Carat Weight\"))                -0.0242      0.011     -2.116      0.034      -0.047      -0.002\n",
      "Color[T.F]:np.log(Q(\"Carat Weight\"))                -0.0086      0.011     -0.797      0.425      -0.030       0.012\n",
      "Color[T.G]:np.log(Q(\"Carat Weight\"))                -0.0474      0.010     -4.689      0.000      -0.067      -0.028\n",
      "Color[T.H]:np.log(Q(\"Carat Weight\"))                -0.1433      0.011    -13.587      0.000      -0.164      -0.123\n",
      "Color[T.I]:np.log(Q(\"Carat Weight\"))                -0.1756      0.011    -16.154      0.000      -0.197      -0.154\n",
      "Cut[T.Good]:np.log(Q(\"Carat Weight\"))                0.0371      0.031      1.208      0.227      -0.023       0.097\n",
      "Cut[T.Ideal]:np.log(Q(\"Carat Weight\"))               0.0012      0.031      0.041      0.968      -0.059       0.061\n",
      "Cut[T.Signature-Ideal]:np.log(Q(\"Carat Weight\"))    -0.0102      0.036     -0.281      0.778      -0.081       0.061\n",
      "Cut[T.Very Good]:np.log(Q(\"Carat Weight\"))           0.0257      0.030      0.851      0.395      -0.033       0.085\n",
      "Report[T.GIA]:np.log(Q(\"Carat Weight\"))             -0.0258      0.016     -1.598      0.110      -0.057       0.006\n",
      "np.maximum(Q(\"Carat Weight\") - 1, 0)                -1.3466      0.031    -43.825      0.000      -1.407      -1.286\n",
      "np.maximum(Q(\"Carat Weight\") - 2, 0)                -0.6823      0.019    -35.662      0.000      -0.720      -0.645\n",
      "==============================================================================\n",
      "Omnibus:                      593.957   Durbin-Watson:                   1.969\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             4086.434\n",
      "Skew:                          -0.193   Prob(JB):                         0.00\n",
      "Kurtosis:                       7.025   Cond. No.                     1.39e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.81e-28. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "Mean Absolute Error for validation data: 600.3999407474377\n",
      "Saved to train_with_predictions.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Fit the model using train data\n",
    "formula = 'np.log(Price_numeric) ~ Q(\"Carat Weight\") + Cut + Clarity + Report + Color + Symmetry + Polish + Polish * Clarity + Polish * Cut + Polish * Symmetry + Polish * Color + Clarity * Color + Clarity * Cut + Clarity * Symmetry + Symmetry * Color + Symmetry * Cut + Color * Cut + np.log(Q(\"Carat Weight\")) + Symmetry*np.log(Q(\"Carat Weight\")) + Polish*np.log(Q(\"Carat Weight\")) + Color*np.log(Q(\"Carat Weight\")) + Cut*np.log(Q(\"Carat Weight\")) + np.maximum(Q(\"Carat Weight\") - 1, 0) + np.maximum(Q(\"Carat Weight\") - 2, 0) + Report*np.log(Q(\"Carat Weight\"))'\n",
    "lm_9 = smf.ols(formula=formula, data=df_train).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(lm_9.summary())\n",
    "\n",
    "# Step 2: Calculate predictions for train data and add them as a new column\n",
    "df_train['Price_Predictions'] = np.exp(lm_9.predict(df_train)) # Using np.exp() to convert log predictions back to original scale\n",
    "\n",
    "# If test_df has a 'Price_numeric' column, we can make predictions for it as well:\n",
    "if 'Price_numeric' in test_df.columns:\n",
    "    test_df['Price_Predictions'] = np.exp(lm_9.predict(test_df))\n",
    "\n",
    "# Step 3: Compute mean absolute error for validation data\n",
    "# Assuming you have a separate validation set named df_validation with actual price information\n",
    "if 'Price_numeric' in df_validation.columns:\n",
    "    lm_9_predictions = lm_9.predict(df_validation)\n",
    "    error = mean_absolute_error(df_validation[\"Price_numeric\"], np.exp(lm_9_predictions))\n",
    "    print(f\"Mean Absolute Error for validation data: {error}\")\n",
    "\n",
    "# Step 4: Combine train and test dataframes\n",
    "combined_df = pd.concat([df_train, test_df], ignore_index=True)\n",
    "\n",
    "# Step 5: Save the combined dataframe to \"train_with_predictions.csv\"\n",
    "combined_df.to_csv(\"train_with_predictions.csv\", index=False)\n",
    "\n",
    "print(\"Saved to train_with_predictions.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fda2fff-7551-40e0-a819-1a99876de316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97066ef2-4383-4d8b-bdf1-6d3c8d5e130d",
   "metadata": {},
   "source": [
    "## Submitting Final Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72289cd7-ff81-49ff-a709-010df2286117",
   "metadata": {},
   "source": [
    "Once you have a model that you are happy with, you should predict on the test set (`df_test`), and then you should submit your predictions to Kaggle (following the instructions in your assignment sheet)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb7b686-d4a4-4e58-a1af-8e1b93b84eeb",
   "metadata": {},
   "source": [
    "For the purpose of showing an example of how we do this, I will assume that you are selecting `lm_1` for your final model. You should obviously change the code to use the model that you would like to submit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed910f2-4eba-4d94-9437-ac4f082586f1",
   "metadata": {},
   "source": [
    "First, we will get the predictions on the test set. Make sure that you predict on the test set (not the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bd3fdfd8-c27e-4b28-9842-4a5dec6e0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = lm_1.predict(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bbef37-679c-4e8d-a6ee-736022d8994b",
   "metadata": {},
   "source": [
    "Now, we can write the predictions to a csv file. We are going to use the `.to_csv()` method. We will give it a filename to write to and a `header` parameter that will tell it what to name the predicted price column. Below we name the file \"DiamondSubmission.csv\", but you can choose a different name if you would like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "83e8d8bd-f68a-4706-a7db-f772d59bd34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions.to_csv(\"DiamondSubmission.csv\", header=[\"Price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a91fe27-d5fc-43b8-8691-f36a9ff99b33",
   "metadata": {},
   "source": [
    "Once you write out the file, you should see it in your file explorer sidebar (it may take a second to appear) in the folder for this class session. You can then download the file by right clicking on it and selecting download. The csv file should have two columns, the first column should be called \"ID\" and the second column should be called \"Price\". There should be 3142 predictions in your dataset (or 3143 rows with the header row)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e52916-7b5a-4fac-9036-a72d60d1a38f",
   "metadata": {},
   "source": [
    "The final step is to submit your predictions to Kaggle! Follow the instructions in the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33c7903-f587-48b4-bbe5-59de1b740542",
   "metadata": {},
   "source": [
    "### Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b208dd0f-f0b7-4441-a79d-104f8bf55084",
   "metadata": {},
   "source": [
    "In this notebook, we only trained our model on `df_smaller_train`. We used `df_validation` to select the model by measuring the performance on a held out validation set. However, after we have selected our model, there is no reason to not retrain on all of our data. Generally speaking, if a model is the best model after being trained on the smaller training set, you can feel pretty confident that it will be the best model if trained on the full training set.\n",
    "\n",
    "The intuition here is that the more data a model has to learn, the better the model is. Once you have identified a set of variables and parameters that make up a good model, you might as well give it all of the data so that it can perform even better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5bb7c1-ea66-4cbb-b6fa-0bc46927a51b",
   "metadata": {},
   "source": [
    "We're not going to walk through this in the notebook, but try to figure out how to train your selected model on the full training set (`df_train`) and then use this model trained on the full training set to make your predictions on the testing set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb281caa4797677f8ea6ae1f44f5e39ac3bca17e340910f273999654c36eecac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
